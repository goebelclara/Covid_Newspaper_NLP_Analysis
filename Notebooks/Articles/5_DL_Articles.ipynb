{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Chapter 1](#chapter1): Transfer Learning with Pre-trained Models\n",
    "    * [Section 1.1](#section_1_1): Guhr\n",
    "    * [Section 1.2](#section_1_2): Mdraw\n",
    "    * [Section 1.3](#section_1_3): Nlptown\n",
    "    * [Section 1.4](#section_1_4): PyABSA\n",
    "* [Chapter 2](#chapter2): Fine-tuning of Best-Performing Pre-trained Model\n",
    "* [Chapter 3](#chapter3): Deep Learning from Scratch\n",
    "    * [Section 3.1](#section_3_1): Sequential Neural Network with Self-trained Embeddings\n",
    "* [Chapter 4](#chapter4): Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: <a class=\"anchor\" id=\"chapter1\"></a> Transfer Learning with Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing required installations\n",
    "#pip install germansentiment\n",
    "#pip install pyabsa\n",
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This script could only be used to manage NVIDIA GPUs,but no GPU found in your device\n",
      "Remote ABSADataset version: 2022.06.10 Local ABSADatasets version: None\n",
      "Unknown local version for ABSADatasets, please check the latest version of ABSADatasets at https://github.com/yangheng95/ABSADatasets\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries\n",
    "#Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "#Sentiment analysis\n",
    "from germansentiment import SentimentModel\n",
    "\n",
    "#Machine and deep learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline, BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from pyabsa import APCCheckpointManager, available_checkpoints\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "import tensorflow\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "#Other\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/Python3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#Suppressing warnings\n",
    "warnings.simplefilter(action = \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in CSVs\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df = pd.read_csv(\"Outputs/Articles/Snorkel/snorkel.csv\", index_col = 0, parse_dates = [\"pubtime\", \"pubday\", \"pubmonth\"])\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping duplicates\n",
    "df_unique = df.drop_duplicates(subset = [\"sentence_ABSA\", \"entity_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting X and y\n",
    "X = df_unique[[\"clause_ABSA\", \"entity_name\"]]\n",
    "y = df_unique[[\"sentiment\", \"manual_annotation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing clause for PYABSA\n",
    "X[\"clause_PYABSA\"] = X.apply(lambda x: x[\"clause_ABSA\"].replace(x[\"entity_name\"].lower(), \n",
    "                                                                \"[ASP]\" + x[\"entity_name\"].lower() + \"[ASP]\"), \n",
    "                             axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting problem to negative sentiment classification only\n",
    "y[\"sentiment\"] = y[\"sentiment\"].apply(lambda x: 0 if x == 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping negative sentiment to 1 instead of -1\n",
    "y[\"sentiment\"] = y[\"sentiment\"].apply(lambda x: 1 if x == -1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting manual annotations between training and testing set\n",
    "X_train_total_manual, X_test_manual, y_train_total_manual, y_test_manual = train_test_split(X[y[\"manual_annotation\"] == 1], \n",
    "                                                                                            y[y[\"manual_annotation\"] == 1], \n",
    "                                                                                            test_size = 0.5, \n",
    "                                                                                            random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting Snorkel annotations between training and testing set\n",
    "X_train_total, X_test, y_train_total, y_test = train_test_split(X[y[\"manual_annotation\"] != 1], \n",
    "                                                                y[y[\"manual_annotation\"] != 1], \n",
    "                                                                test_size = 0.2, \n",
    "                                                                random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging datasets\n",
    "X_train_total = pd.concat([X_train_total, X_train_total_manual])\n",
    "X_test = pd.concat([X_test, X_test_manual])\n",
    "y_train_total = pd.concat([y_train_total, y_train_total_manual])\n",
    "y_test = pd.concat([y_test, y_test_manual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting merged datasets between training and validation set\n",
    "X_train, X_vali, y_train, y_vali = train_test_split(X_train_total, \n",
    "                                                    y_train_total, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting gold label sets\n",
    "X_train_gold = X_train[y_train[\"manual_annotation\"] == 1]\n",
    "y_train_gold = y_train[y_train[\"manual_annotation\"] == 1]\n",
    "\n",
    "X_vali_gold = X_vali[y_vali[\"manual_annotation\"] == 1]\n",
    "y_vali_gold = y_vali[y_vali[\"manual_annotation\"] == 1]\n",
    "\n",
    "X_test_gold = X_test[y_test[\"manual_annotation\"] == 1]\n",
    "y_test_gold = y_test[y_test[\"manual_annotation\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1: <a class=\"anchor\" id=\"section_1_1\"></a> Guhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting sentiment map\n",
    "sentiment_map = {\"1\": 0, \"0\": 0, \"2\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting sentiment label map\n",
    "sentiment_label_map = {\"negative\": 1, \"positive\": 0, \"neutral\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining class for Guhr model\n",
    "#https://huggingface.co/oliverguhr/german-sentiment-bert\n",
    "class SentimentModel_guhr():\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.clean_chars = re.compile(r'[^A-Za-züöäÖÜÄß ]', re.MULTILINE)\n",
    "        \n",
    "    def predict_sentiment_batch(self, texts: List[str], polarity_label) -> List[str]:\n",
    "        texts = [self.clean_text(text) for text in texts]\n",
    "        encoded = self.tokenizer.batch_encode_plus(texts, \n",
    "                                                   padding = True, \n",
    "                                                   add_special_tokens = True,\n",
    "                                                   truncation = True, \n",
    "                                                   return_tensors = \"pt\")\n",
    "        #encoded = encoded.to(self.device)\n",
    "        with torch.no_grad():\n",
    "                logits = self.model(**encoded)\n",
    "        \n",
    "        label_ids = torch.argmax(logits[0], axis = 1)\n",
    "        \n",
    "        polarity_labels = [self.model.config.id2label[label_id.item()] for label_id in label_ids]\n",
    "        if polarity_label == True:\n",
    "            return polarity_labels\n",
    "        else:\n",
    "            polarities = [sentiment_label_map[x] for x in polarity_labels]\n",
    "            return polarities\n",
    "    \n",
    "    def analyse_sentiment(self, text: str, polarity_label) -> List[str]:\n",
    "        polarity_label = self.predict_sentiment_batch([text], polarity_label = True)[0]\n",
    "        \n",
    "        if polarity_label == True:\n",
    "            return polarity_label\n",
    "        else:\n",
    "            polarity = sentiment_label_map[polarity_label]\n",
    "            return polarity\n",
    "\n",
    "    def clean_text(self,text: str) -> str:    \n",
    "        text = text.replace(\"\\n\", \" \")        \n",
    "        text = self.clean_chars.sub(\"\", text)                       \n",
    "        text = \" \".join(text.split())   \n",
    "        text = text.strip().lower()\n",
    "        return text\n",
    "\n",
    "model_guhr = SentimentModel_guhr(model_name = \"oliverguhr/german-sentiment-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to create batches\n",
    "def create_batches(df, batch_size = 100): \n",
    "    batches = list()\n",
    "    num_batches = len(df) // batch_size + 1\n",
    "    for i in range(num_batches):\n",
    "        batches.append(df[i*batch_size:(i+1)*batch_size])\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to predict batches\n",
    "def predict_batches_guhr(batches, indeces): \n",
    "    preds = pd.Series()\n",
    "    start = time.time()\n",
    "    for index, batch in enumerate(batches):\n",
    "        pred = model_guhr.predict_sentiment_batch(list(batch), False)\n",
    "        pred = pd.Series(pred)\n",
    "        preds = pd.concat([preds, pred])\n",
    "        print(f\"Batch: {index}\")\n",
    "    preds = pd.Series(data = preds.values, index = indeces)\n",
    "    end = time.time()\n",
    "    hours = (end - start)/60/60\n",
    "    print(\"Hours elapsed: \", round(hours,1))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating batches\n",
    "batches_X_test = create_batches(X_test[\"clause_ABSA\"], batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Batch: 1\n",
      "Batch: 2\n",
      "Batch: 3\n",
      "Batch: 4\n",
      "Batch: 5\n",
      "Batch: 6\n",
      "Batch: 7\n",
      "Batch: 8\n",
      "Batch: 9\n",
      "Batch: 10\n",
      "Batch: 11\n",
      "Batch: 12\n",
      "Batch: 13\n",
      "Batch: 14\n",
      "Batch: 15\n",
      "Batch: 16\n",
      "Batch: 17\n",
      "Batch: 18\n",
      "Batch: 19\n",
      "Batch: 20\n",
      "Batch: 21\n",
      "Batch: 22\n",
      "Batch: 23\n",
      "Batch: 24\n",
      "Batch: 25\n",
      "Batch: 26\n",
      "Batch: 27\n",
      "Batch: 28\n",
      "Batch: 29\n",
      "Batch: 30\n",
      "Batch: 31\n",
      "Batch: 32\n",
      "Batch: 33\n",
      "Batch: 34\n",
      "Batch: 35\n",
      "Batch: 36\n",
      "Batch: 37\n",
      "Batch: 38\n",
      "Batch: 39\n",
      "Batch: 40\n",
      "Batch: 41\n",
      "Batch: 42\n",
      "Batch: 43\n",
      "Batch: 44\n",
      "Batch: 45\n",
      "Batch: 46\n",
      "Batch: 47\n",
      "Batch: 48\n",
      "Batch: 49\n",
      "Batch: 50\n",
      "Batch: 51\n",
      "Batch: 52\n",
      "Batch: 53\n",
      "Batch: 54\n",
      "Batch: 55\n",
      "Batch: 56\n",
      "Batch: 57\n",
      "Batch: 58\n",
      "Batch: 59\n",
      "Batch: 60\n",
      "Batch: 61\n",
      "Batch: 62\n",
      "Batch: 63\n",
      "Batch: 64\n",
      "Batch: 65\n",
      "Batch: 66\n",
      "Batch: 67\n",
      "Batch: 68\n",
      "Batch: 69\n",
      "Batch: 70\n",
      "Batch: 71\n",
      "Batch: 72\n",
      "Batch: 73\n",
      "Batch: 74\n",
      "Batch: 75\n",
      "Batch: 76\n",
      "Batch: 77\n",
      "Batch: 78\n",
      "Batch: 79\n",
      "Batch: 80\n",
      "Batch: 81\n",
      "Batch: 82\n",
      "Batch: 83\n",
      "Batch: 84\n",
      "Batch: 85\n",
      "Batch: 86\n",
      "Batch: 87\n",
      "Batch: 88\n",
      "Batch: 89\n",
      "Batch: 90\n",
      "Batch: 91\n",
      "Batch: 92\n",
      "Batch: 93\n",
      "Batch: 94\n",
      "Batch: 95\n",
      "Batch: 96\n",
      "Batch: 97\n",
      "Batch: 98\n",
      "Batch: 99\n",
      "Batch: 100\n",
      "Batch: 101\n",
      "Batch: 102\n",
      "Batch: 103\n",
      "Batch: 104\n",
      "Batch: 105\n",
      "Batch: 106\n",
      "Batch: 107\n",
      "Batch: 108\n",
      "Batch: 109\n",
      "Batch: 110\n",
      "Batch: 111\n",
      "Batch: 112\n",
      "Batch: 113\n",
      "Batch: 114\n",
      "Batch: 115\n",
      "Batch: 116\n",
      "Batch: 117\n",
      "Batch: 118\n",
      "Batch: 119\n",
      "Batch: 120\n",
      "Batch: 121\n",
      "Batch: 122\n",
      "Batch: 123\n",
      "Batch: 124\n",
      "Batch: 125\n",
      "Batch: 126\n",
      "Batch: 127\n",
      "Batch: 128\n",
      "Batch: 129\n",
      "Batch: 130\n",
      "Batch: 131\n",
      "Batch: 132\n",
      "Batch: 133\n",
      "Batch: 134\n",
      "Batch: 135\n",
      "Batch: 136\n",
      "Batch: 137\n",
      "Batch: 138\n",
      "Batch: 139\n",
      "Batch: 140\n",
      "Batch: 141\n",
      "Batch: 142\n",
      "Batch: 143\n",
      "Batch: 144\n",
      "Batch: 145\n",
      "Batch: 146\n",
      "Batch: 147\n",
      "Batch: 148\n",
      "Batch: 149\n",
      "Batch: 150\n",
      "Batch: 151\n",
      "Batch: 152\n",
      "Batch: 153\n",
      "Batch: 154\n",
      "Batch: 155\n",
      "Batch: 156\n",
      "Batch: 157\n",
      "Batch: 158\n",
      "Batch: 159\n",
      "Batch: 160\n",
      "Batch: 161\n",
      "Batch: 162\n",
      "Batch: 163\n",
      "Batch: 164\n",
      "Batch: 165\n",
      "Batch: 166\n",
      "Batch: 167\n",
      "Batch: 168\n",
      "Batch: 169\n",
      "Batch: 170\n",
      "Batch: 171\n",
      "Batch: 172\n",
      "Batch: 173\n",
      "Batch: 174\n",
      "Batch: 175\n",
      "Batch: 176\n",
      "Batch: 177\n",
      "Batch: 178\n",
      "Batch: 179\n",
      "Batch: 180\n",
      "Batch: 181\n",
      "Batch: 182\n",
      "Batch: 183\n",
      "Batch: 184\n",
      "Batch: 185\n",
      "Batch: 186\n",
      "Batch: 187\n",
      "Batch: 188\n",
      "Batch: 189\n",
      "Batch: 190\n",
      "Batch: 191\n",
      "Batch: 192\n",
      "Batch: 193\n",
      "Batch: 194\n",
      "Batch: 195\n",
      "Batch: 196\n",
      "Batch: 197\n",
      "Batch: 198\n",
      "Batch: 199\n",
      "Batch: 200\n",
      "Batch: 201\n",
      "Batch: 202\n",
      "Batch: 203\n",
      "Batch: 204\n",
      "Batch: 205\n",
      "Batch: 206\n",
      "Batch: 207\n",
      "Batch: 208\n",
      "Batch: 209\n",
      "Batch: 210\n",
      "Batch: 211\n",
      "Batch: 212\n",
      "Batch: 213\n",
      "Batch: 214\n",
      "Batch: 215\n",
      "Batch: 216\n",
      "Batch: 217\n",
      "Batch: 218\n",
      "Batch: 219\n",
      "Batch: 220\n",
      "Batch: 221\n",
      "Batch: 222\n",
      "Batch: 223\n",
      "Batch: 224\n",
      "Batch: 225\n",
      "Batch: 226\n",
      "Batch: 227\n",
      "Batch: 228\n",
      "Batch: 229\n",
      "Batch: 230\n",
      "Batch: 231\n",
      "Batch: 232\n",
      "Batch: 233\n",
      "Batch: 234\n",
      "Batch: 235\n",
      "Batch: 236\n",
      "Batch: 237\n",
      "Batch: 238\n",
      "Batch: 239\n",
      "Batch: 240\n",
      "Batch: 241\n",
      "Batch: 242\n",
      "Batch: 243\n",
      "Batch: 244\n",
      "Batch: 245\n",
      "Batch: 246\n",
      "Batch: 247\n",
      "Batch: 248\n",
      "Batch: 249\n",
      "Batch: 250\n",
      "Batch: 251\n",
      "Batch: 252\n",
      "Batch: 253\n",
      "Batch: 254\n",
      "Batch: 255\n",
      "Batch: 256\n",
      "Batch: 257\n",
      "Batch: 258\n",
      "Batch: 259\n",
      "Batch: 260\n",
      "Batch: 261\n",
      "Batch: 262\n",
      "Batch: 263\n",
      "Batch: 264\n",
      "Batch: 265\n",
      "Batch: 266\n",
      "Batch: 267\n",
      "Batch: 268\n",
      "Batch: 269\n",
      "Batch: 270\n",
      "Batch: 271\n",
      "Batch: 272\n",
      "Batch: 273\n",
      "Batch: 274\n",
      "Batch: 275\n",
      "Batch: 276\n",
      "Batch: 277\n",
      "Batch: 278\n",
      "Batch: 279\n",
      "Batch: 280\n",
      "Batch: 281\n",
      "Batch: 282\n",
      "Batch: 283\n",
      "Batch: 284\n",
      "Batch: 285\n",
      "Batch: 286\n",
      "Batch: 287\n",
      "Batch: 288\n",
      "Batch: 289\n",
      "Batch: 290\n",
      "Batch: 291\n",
      "Batch: 292\n",
      "Batch: 293\n",
      "Batch: 294\n",
      "Batch: 295\n",
      "Batch: 296\n",
      "Batch: 297\n",
      "Batch: 298\n",
      "Batch: 299\n",
      "Batch: 300\n",
      "Batch: 301\n",
      "Batch: 302\n",
      "Batch: 303\n",
      "Batch: 304\n",
      "Batch: 305\n",
      "Batch: 306\n",
      "Batch: 307\n",
      "Batch: 308\n",
      "Batch: 309\n",
      "Batch: 310\n",
      "Batch: 311\n",
      "Batch: 312\n",
      "Batch: 313\n",
      "Batch: 314\n",
      "Batch: 315\n",
      "Batch: 316\n",
      "Batch: 317\n",
      "Batch: 318\n",
      "Batch: 319\n",
      "Batch: 320\n",
      "Batch: 321\n",
      "Batch: 322\n",
      "Batch: 323\n",
      "Batch: 324\n",
      "Batch: 325\n",
      "Batch: 326\n",
      "Batch: 327\n",
      "Batch: 328\n",
      "Batch: 329\n",
      "Batch: 330\n",
      "Batch: 331\n",
      "Batch: 332\n",
      "Batch: 333\n",
      "Batch: 334\n",
      "Batch: 335\n",
      "Hours elapsed:  0.2\n"
     ]
    }
   ],
   "source": [
    "#Predicting batches\n",
    "y_test_pred_guhr = predict_batches_guhr(batches_X_test, list(X_test[\"clause_ABSA\"].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.94     30212\n",
      "           1       0.15      0.05      0.07      3310\n",
      "\n",
      "    accuracy                           0.88     33522\n",
      "   macro avg       0.53      0.51      0.50     33522\n",
      "weighted avg       0.83      0.88      0.85     33522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating predictions on total test set\n",
    "print(classification_report(y_test[\"sentiment\"], y_test_pred_guhr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2: <a class=\"anchor\" id=\"section_1_2\"></a> Mdraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining class for mdraw model\n",
    "#https://huggingface.co/mdraw/german-news-sentiment-bert\n",
    "class SentimentModel_mdraw:\n",
    "    def __init__(self, model_name: str = \"oliverguhr/german-sentiment-bert\"):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"oliverguhr/german-sentiment-bert\")\n",
    "        self.clean_chars = re.compile(r'[^A-Za-züöäÖÜÄß ]', re.MULTILINE)\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        text = self.clean_chars.sub(\"\", text)\n",
    "        text = \" \".join(text.split())\n",
    "        text = text.strip().lower()\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def probs2polarities(pnn: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Transform softmax probs of a [positive, negative, neutral] classifier\n",
    "        into scalar polarity scores of range [-1, 1].\"\"\"\n",
    "        pos = pnn[:, 0]\n",
    "        neg = pnn[:, 1]\n",
    "        polarities = pos - neg\n",
    "        return polarities\n",
    "\n",
    "    def sort_polarity(self, polarity_score):\n",
    "        if polarity_score > 0.5:\n",
    "            return 1\n",
    "        elif polarity_score < -0.5:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def predict_sentiment_batch(self, texts: List[str], polarity_score) -> torch.Tensor:\n",
    "        texts = [self.clean_text(text) for text in texts]\n",
    "        input_ids = self.tokenizer.batch_encode_plus(texts,\n",
    "                                                     padding = True,\n",
    "                                                     add_special_tokens = True,\n",
    "                                                     truncation = True)\n",
    "        input_ids = torch.tensor(input_ids[\"input_ids\"])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids)\n",
    "            probs = F.softmax(logits[0], dim = 1)\n",
    "\n",
    "        polarity_scores = self.probs2polarities(probs)\n",
    "        \n",
    "        if polarity_score == True:\n",
    "            return polarity_scores\n",
    "        else:\n",
    "            polarities = [self.sort_polarity(x) for x in polarity_scores]\n",
    "            polarities = [sentiment_map[str(x)] for x in polarities]\n",
    "            return polarities\n",
    "\n",
    "    def analyse_sentiment(self, text: str, polarity_score) -> float:\n",
    "        polarity_score = self.predict_sentiment_batch([text], polarity_score = True).item()\n",
    "        \n",
    "        if polarity_score == True:\n",
    "            return polarity_score\n",
    "        else:\n",
    "            polarity = self.sort_polarity(polarity_score)\n",
    "            polarity = sentiment_map[str(polarity)]\n",
    "            return polarity\n",
    "\n",
    "model_mdraw = SentimentModel_mdraw(\"mdraw/german-news-sentiment-bert\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to predict batches\n",
    "def predict_batches_mdraw(batches, indeces): \n",
    "    preds = pd.Series()\n",
    "    start = time.time()\n",
    "    for index, batch in enumerate(batches):\n",
    "        pred = model_mdraw.predict_sentiment_batch(batch, False)\n",
    "        pred = pd.Series(pred)\n",
    "        preds = pd.concat([preds, pred])\n",
    "        print(f\"Batch: {index}\")\n",
    "    preds = pd.Series(data = preds.values, index = indeces)\n",
    "    end = time.time()\n",
    "    hours = (end - start)/60/60\n",
    "    print(\"Hours elapsed: \", round(hours,1))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Batch: 1\n",
      "Batch: 2\n",
      "Batch: 3\n",
      "Batch: 4\n",
      "Batch: 5\n",
      "Batch: 6\n",
      "Batch: 7\n",
      "Batch: 8\n",
      "Batch: 9\n",
      "Batch: 10\n",
      "Batch: 11\n",
      "Batch: 12\n",
      "Batch: 13\n",
      "Batch: 14\n",
      "Batch: 15\n",
      "Batch: 16\n",
      "Batch: 17\n",
      "Batch: 18\n",
      "Batch: 19\n",
      "Batch: 20\n",
      "Batch: 21\n",
      "Batch: 22\n",
      "Batch: 23\n",
      "Batch: 24\n",
      "Batch: 25\n",
      "Batch: 26\n",
      "Batch: 27\n",
      "Batch: 28\n",
      "Batch: 29\n",
      "Batch: 30\n",
      "Batch: 31\n",
      "Batch: 32\n",
      "Batch: 33\n",
      "Batch: 34\n",
      "Batch: 35\n",
      "Batch: 36\n",
      "Batch: 37\n",
      "Batch: 38\n",
      "Batch: 39\n",
      "Batch: 40\n",
      "Batch: 41\n",
      "Batch: 42\n",
      "Batch: 43\n",
      "Batch: 44\n",
      "Batch: 45\n",
      "Batch: 46\n",
      "Batch: 47\n",
      "Batch: 48\n",
      "Batch: 49\n",
      "Batch: 50\n",
      "Batch: 51\n",
      "Batch: 52\n",
      "Batch: 53\n",
      "Batch: 54\n",
      "Batch: 55\n",
      "Batch: 56\n",
      "Batch: 57\n",
      "Batch: 58\n",
      "Batch: 59\n",
      "Batch: 60\n",
      "Batch: 61\n",
      "Batch: 62\n",
      "Batch: 63\n",
      "Batch: 64\n",
      "Batch: 65\n",
      "Batch: 66\n",
      "Batch: 67\n",
      "Batch: 68\n",
      "Batch: 69\n",
      "Batch: 70\n",
      "Batch: 71\n",
      "Batch: 72\n",
      "Batch: 73\n",
      "Batch: 74\n",
      "Batch: 75\n",
      "Batch: 76\n",
      "Batch: 77\n",
      "Batch: 78\n",
      "Batch: 79\n",
      "Batch: 80\n",
      "Batch: 81\n",
      "Batch: 82\n",
      "Batch: 83\n",
      "Batch: 84\n",
      "Batch: 85\n",
      "Batch: 86\n",
      "Batch: 87\n",
      "Batch: 88\n",
      "Batch: 89\n",
      "Batch: 90\n",
      "Batch: 91\n",
      "Batch: 92\n",
      "Batch: 93\n",
      "Batch: 94\n",
      "Batch: 95\n",
      "Batch: 96\n",
      "Batch: 97\n",
      "Batch: 98\n",
      "Batch: 99\n",
      "Batch: 100\n",
      "Batch: 101\n",
      "Batch: 102\n",
      "Batch: 103\n",
      "Batch: 104\n",
      "Batch: 105\n",
      "Batch: 106\n",
      "Batch: 107\n",
      "Batch: 108\n",
      "Batch: 109\n",
      "Batch: 110\n",
      "Batch: 111\n",
      "Batch: 112\n",
      "Batch: 113\n",
      "Batch: 114\n",
      "Batch: 115\n",
      "Batch: 116\n",
      "Batch: 117\n",
      "Batch: 118\n",
      "Batch: 119\n",
      "Batch: 120\n",
      "Batch: 121\n",
      "Batch: 122\n",
      "Batch: 123\n",
      "Batch: 124\n",
      "Batch: 125\n",
      "Batch: 126\n",
      "Batch: 127\n",
      "Batch: 128\n",
      "Batch: 129\n",
      "Batch: 130\n",
      "Batch: 131\n",
      "Batch: 132\n",
      "Batch: 133\n",
      "Batch: 134\n",
      "Batch: 135\n",
      "Batch: 136\n",
      "Batch: 137\n",
      "Batch: 138\n",
      "Batch: 139\n",
      "Batch: 140\n",
      "Batch: 141\n",
      "Batch: 142\n",
      "Batch: 143\n",
      "Batch: 144\n",
      "Batch: 145\n",
      "Batch: 146\n",
      "Batch: 147\n",
      "Batch: 148\n",
      "Batch: 149\n",
      "Batch: 150\n",
      "Batch: 151\n",
      "Batch: 152\n",
      "Batch: 153\n",
      "Batch: 154\n",
      "Batch: 155\n",
      "Batch: 156\n",
      "Batch: 157\n",
      "Batch: 158\n",
      "Batch: 159\n",
      "Batch: 160\n",
      "Batch: 161\n",
      "Batch: 162\n",
      "Batch: 163\n",
      "Batch: 164\n",
      "Batch: 165\n",
      "Batch: 166\n",
      "Batch: 167\n",
      "Batch: 168\n",
      "Batch: 169\n",
      "Batch: 170\n",
      "Batch: 171\n",
      "Batch: 172\n",
      "Batch: 173\n",
      "Batch: 174\n",
      "Batch: 175\n",
      "Batch: 176\n",
      "Batch: 177\n",
      "Batch: 178\n",
      "Batch: 179\n",
      "Batch: 180\n",
      "Batch: 181\n",
      "Batch: 182\n",
      "Batch: 183\n",
      "Batch: 184\n",
      "Batch: 185\n",
      "Batch: 186\n",
      "Batch: 187\n",
      "Batch: 188\n",
      "Batch: 189\n",
      "Batch: 190\n",
      "Batch: 191\n",
      "Batch: 192\n",
      "Batch: 193\n",
      "Batch: 194\n",
      "Batch: 195\n",
      "Batch: 196\n",
      "Batch: 197\n",
      "Batch: 198\n",
      "Batch: 199\n",
      "Batch: 200\n",
      "Batch: 201\n",
      "Batch: 202\n",
      "Batch: 203\n",
      "Batch: 204\n",
      "Batch: 205\n",
      "Batch: 206\n",
      "Batch: 207\n",
      "Batch: 208\n",
      "Batch: 209\n",
      "Batch: 210\n",
      "Batch: 211\n",
      "Batch: 212\n",
      "Batch: 213\n",
      "Batch: 214\n",
      "Batch: 215\n",
      "Batch: 216\n",
      "Batch: 217\n",
      "Batch: 218\n",
      "Batch: 219\n",
      "Batch: 220\n",
      "Batch: 221\n",
      "Batch: 222\n",
      "Batch: 223\n",
      "Batch: 224\n",
      "Batch: 225\n",
      "Batch: 226\n",
      "Batch: 227\n",
      "Batch: 228\n",
      "Batch: 229\n",
      "Batch: 230\n",
      "Batch: 231\n",
      "Batch: 232\n",
      "Batch: 233\n",
      "Batch: 234\n",
      "Batch: 235\n",
      "Batch: 236\n",
      "Batch: 237\n",
      "Batch: 238\n",
      "Batch: 239\n",
      "Batch: 240\n",
      "Batch: 241\n",
      "Batch: 242\n",
      "Batch: 243\n",
      "Batch: 244\n",
      "Batch: 245\n",
      "Batch: 246\n",
      "Batch: 247\n",
      "Batch: 248\n",
      "Batch: 249\n",
      "Batch: 250\n",
      "Batch: 251\n",
      "Batch: 252\n",
      "Batch: 253\n",
      "Batch: 254\n",
      "Batch: 255\n",
      "Batch: 256\n",
      "Batch: 257\n",
      "Batch: 258\n",
      "Batch: 259\n",
      "Batch: 260\n",
      "Batch: 261\n",
      "Batch: 262\n",
      "Batch: 263\n",
      "Batch: 264\n",
      "Batch: 265\n",
      "Batch: 266\n",
      "Batch: 267\n",
      "Batch: 268\n",
      "Batch: 269\n",
      "Batch: 270\n",
      "Batch: 271\n",
      "Batch: 272\n",
      "Batch: 273\n",
      "Batch: 274\n",
      "Batch: 275\n",
      "Batch: 276\n",
      "Batch: 277\n",
      "Batch: 278\n",
      "Batch: 279\n",
      "Batch: 280\n",
      "Batch: 281\n",
      "Batch: 282\n",
      "Batch: 283\n",
      "Batch: 284\n",
      "Batch: 285\n",
      "Batch: 286\n",
      "Batch: 287\n",
      "Batch: 288\n",
      "Batch: 289\n",
      "Batch: 290\n",
      "Batch: 291\n",
      "Batch: 292\n",
      "Batch: 293\n",
      "Batch: 294\n",
      "Batch: 295\n",
      "Batch: 296\n",
      "Batch: 297\n",
      "Batch: 298\n",
      "Batch: 299\n",
      "Batch: 300\n",
      "Batch: 301\n",
      "Batch: 302\n",
      "Batch: 303\n",
      "Batch: 304\n",
      "Batch: 305\n",
      "Batch: 306\n",
      "Batch: 307\n",
      "Batch: 308\n",
      "Batch: 309\n",
      "Batch: 310\n",
      "Batch: 311\n",
      "Batch: 312\n",
      "Batch: 313\n",
      "Batch: 314\n",
      "Batch: 315\n",
      "Batch: 316\n",
      "Batch: 317\n",
      "Batch: 318\n",
      "Batch: 319\n",
      "Batch: 320\n",
      "Batch: 321\n",
      "Batch: 322\n",
      "Batch: 323\n",
      "Batch: 324\n",
      "Batch: 325\n",
      "Batch: 326\n",
      "Batch: 327\n",
      "Batch: 328\n",
      "Batch: 329\n",
      "Batch: 330\n",
      "Batch: 331\n",
      "Batch: 332\n",
      "Batch: 333\n",
      "Batch: 334\n",
      "Batch: 335\n",
      "Hours elapsed:  0.2\n"
     ]
    }
   ],
   "source": [
    "#Predicting batches\n",
    "y_test_pred_mdraw = predict_batches_mdraw(batches_X_test, list(X_test[\"clause_ABSA\"].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.84      0.89     30212\n",
      "           1       0.25      0.48      0.32      3310\n",
      "\n",
      "    accuracy                           0.80     33522\n",
      "   macro avg       0.59      0.66      0.60     33522\n",
      "weighted avg       0.87      0.80      0.83     33522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating predictions on total test set\n",
    "print(classification_report(y_test[\"sentiment\"], y_test_pred_mdraw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3: <a class=\"anchor\" id=\"section_1_3\"></a> Nlptown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining class for nlptown model\n",
    "#https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment\n",
    "class SentimentModel_nlptown:\n",
    "    def __init__(self, model_name: str = \"nlptown/bert-base-multilingual-uncased-sentiment\"):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def calculate_sentiment_from_stars(self, results: List[dict]):\n",
    "        one_star = results[0][\"score\"]\n",
    "        two_star = results[1][\"score\"]\n",
    "        three_star = results[2][\"score\"]\n",
    "        four_star = results[3][\"score\"]\n",
    "        five_star = results[4][\"score\"]\n",
    "        stars_probs = [one_star, two_star, three_star, four_star, five_star]\n",
    "        stars_sentiment = [1, 0, 0, 0, 0]\n",
    "        \n",
    "        if two_star + three_star + four_star > 0.5:\n",
    "            return 0\n",
    "        else:\n",
    "            max_idx = np.argmax(stars_probs)\n",
    "            sentiment = stars_sentiment[max_idx]\n",
    "            return sentiment\n",
    "    \n",
    "    def predict_sentiment_batch(self, texts: List[str]):\n",
    "        pipe = TextClassificationPipeline(model = self.model, tokenizer = self.tokenizer, return_all_scores = True)\n",
    "        pipe_results = pipe(texts)\n",
    "        results = [self.calculate_sentiment_from_stars(x) for x in pipe_results]\n",
    "        return results\n",
    "    \n",
    "    def analyse_sentiment(self, text: str):\n",
    "        sentiment = self.predict_sentiment_batch([text])[0]\n",
    "        return sentiment\n",
    "\n",
    "model_nlptown = SentimentModel_nlptown(\"nlptown/bert-base-multilingual-uncased-sentiment\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to predict batches\n",
    "def predict_batches_nlptown(batches, indeces): \n",
    "    preds = pd.Series()\n",
    "    start = time.time()\n",
    "    for index, batch in enumerate(batches):\n",
    "        pred = model_nlptown.predict_sentiment_batch(list(batch))\n",
    "        pred = pd.Series(pred)\n",
    "        preds = pd.concat([preds, pred])\n",
    "        print(f\"Batch: {index}\")\n",
    "    preds = pd.Series(data = preds.values, index = indeces)\n",
    "    end = time.time()\n",
    "    hours = (end - start)/60/60\n",
    "    print(\"Hours elapsed: \", round(hours,1))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Batch: 1\n",
      "Batch: 2\n",
      "Batch: 3\n",
      "Batch: 4\n",
      "Batch: 5\n",
      "Batch: 6\n",
      "Batch: 7\n",
      "Batch: 8\n",
      "Batch: 9\n",
      "Batch: 10\n",
      "Batch: 11\n",
      "Batch: 12\n",
      "Batch: 13\n",
      "Batch: 14\n",
      "Batch: 15\n",
      "Batch: 16\n",
      "Batch: 17\n",
      "Batch: 18\n",
      "Batch: 19\n",
      "Batch: 20\n",
      "Batch: 21\n",
      "Batch: 22\n",
      "Batch: 23\n",
      "Batch: 24\n",
      "Batch: 25\n",
      "Batch: 26\n",
      "Batch: 27\n",
      "Batch: 28\n",
      "Batch: 29\n",
      "Batch: 30\n",
      "Batch: 31\n",
      "Batch: 32\n",
      "Batch: 33\n",
      "Batch: 34\n",
      "Batch: 35\n",
      "Batch: 36\n",
      "Batch: 37\n",
      "Batch: 38\n",
      "Batch: 39\n",
      "Batch: 40\n",
      "Batch: 41\n",
      "Batch: 42\n",
      "Batch: 43\n",
      "Batch: 44\n",
      "Batch: 45\n",
      "Batch: 46\n",
      "Batch: 47\n",
      "Batch: 48\n",
      "Batch: 49\n",
      "Batch: 50\n",
      "Batch: 51\n",
      "Batch: 52\n",
      "Batch: 53\n",
      "Batch: 54\n",
      "Batch: 55\n",
      "Batch: 56\n",
      "Batch: 57\n",
      "Batch: 58\n",
      "Batch: 59\n",
      "Batch: 60\n",
      "Batch: 61\n",
      "Batch: 62\n",
      "Batch: 63\n",
      "Batch: 64\n",
      "Batch: 65\n",
      "Batch: 66\n",
      "Batch: 67\n",
      "Batch: 68\n",
      "Batch: 69\n",
      "Batch: 70\n",
      "Batch: 71\n",
      "Batch: 72\n",
      "Batch: 73\n",
      "Batch: 74\n",
      "Batch: 75\n",
      "Batch: 76\n",
      "Batch: 77\n",
      "Batch: 78\n",
      "Batch: 79\n",
      "Batch: 80\n",
      "Batch: 81\n",
      "Batch: 82\n",
      "Batch: 83\n",
      "Batch: 84\n",
      "Batch: 85\n",
      "Batch: 86\n",
      "Batch: 87\n",
      "Batch: 88\n",
      "Batch: 89\n",
      "Batch: 90\n",
      "Batch: 91\n",
      "Batch: 92\n",
      "Batch: 93\n",
      "Batch: 94\n",
      "Batch: 95\n",
      "Batch: 96\n",
      "Batch: 97\n",
      "Batch: 98\n",
      "Batch: 99\n",
      "Batch: 100\n",
      "Batch: 101\n",
      "Batch: 102\n",
      "Batch: 103\n",
      "Batch: 104\n",
      "Batch: 105\n",
      "Batch: 106\n",
      "Batch: 107\n",
      "Batch: 108\n",
      "Batch: 109\n",
      "Batch: 110\n",
      "Batch: 111\n",
      "Batch: 112\n",
      "Batch: 113\n",
      "Batch: 114\n",
      "Batch: 115\n",
      "Batch: 116\n",
      "Batch: 117\n",
      "Batch: 118\n",
      "Batch: 119\n",
      "Batch: 120\n",
      "Batch: 121\n",
      "Batch: 122\n",
      "Batch: 123\n",
      "Batch: 124\n",
      "Batch: 125\n",
      "Batch: 126\n",
      "Batch: 127\n",
      "Batch: 128\n",
      "Batch: 129\n",
      "Batch: 130\n",
      "Batch: 131\n",
      "Batch: 132\n",
      "Batch: 133\n",
      "Batch: 134\n",
      "Batch: 135\n",
      "Batch: 136\n",
      "Batch: 137\n",
      "Batch: 138\n",
      "Batch: 139\n",
      "Batch: 140\n",
      "Batch: 141\n",
      "Batch: 142\n",
      "Batch: 143\n",
      "Batch: 144\n",
      "Batch: 145\n",
      "Batch: 146\n",
      "Batch: 147\n",
      "Batch: 148\n",
      "Batch: 149\n",
      "Batch: 150\n",
      "Batch: 151\n",
      "Batch: 152\n",
      "Batch: 153\n",
      "Batch: 154\n",
      "Batch: 155\n",
      "Batch: 156\n",
      "Batch: 157\n",
      "Batch: 158\n",
      "Batch: 159\n",
      "Batch: 160\n",
      "Batch: 161\n",
      "Batch: 162\n",
      "Batch: 163\n",
      "Batch: 164\n",
      "Batch: 165\n",
      "Batch: 166\n",
      "Batch: 167\n",
      "Batch: 168\n",
      "Batch: 169\n",
      "Batch: 170\n",
      "Batch: 171\n",
      "Batch: 172\n",
      "Batch: 173\n",
      "Batch: 174\n",
      "Batch: 175\n",
      "Batch: 176\n",
      "Batch: 177\n",
      "Batch: 178\n",
      "Batch: 179\n",
      "Batch: 180\n",
      "Batch: 181\n",
      "Batch: 182\n",
      "Batch: 183\n",
      "Batch: 184\n",
      "Batch: 185\n",
      "Batch: 186\n",
      "Batch: 187\n",
      "Batch: 188\n",
      "Batch: 189\n",
      "Batch: 190\n",
      "Batch: 191\n",
      "Batch: 192\n",
      "Batch: 193\n",
      "Batch: 194\n",
      "Batch: 195\n",
      "Batch: 196\n",
      "Batch: 197\n",
      "Batch: 198\n",
      "Batch: 199\n",
      "Batch: 200\n",
      "Batch: 201\n",
      "Batch: 202\n",
      "Batch: 203\n",
      "Batch: 204\n",
      "Batch: 205\n",
      "Batch: 206\n",
      "Batch: 207\n",
      "Batch: 208\n",
      "Batch: 209\n",
      "Batch: 210\n",
      "Batch: 211\n",
      "Batch: 212\n",
      "Batch: 213\n",
      "Batch: 214\n",
      "Batch: 215\n",
      "Batch: 216\n",
      "Batch: 217\n",
      "Batch: 218\n",
      "Batch: 219\n",
      "Batch: 220\n",
      "Batch: 221\n",
      "Batch: 222\n",
      "Batch: 223\n",
      "Batch: 224\n",
      "Batch: 225\n",
      "Batch: 226\n",
      "Batch: 227\n",
      "Batch: 228\n",
      "Batch: 229\n",
      "Batch: 230\n",
      "Batch: 231\n",
      "Batch: 232\n",
      "Batch: 233\n",
      "Batch: 234\n",
      "Batch: 235\n",
      "Batch: 236\n",
      "Batch: 237\n",
      "Batch: 238\n",
      "Batch: 239\n",
      "Batch: 240\n",
      "Batch: 241\n",
      "Batch: 242\n",
      "Batch: 243\n",
      "Batch: 244\n",
      "Batch: 245\n",
      "Batch: 246\n",
      "Batch: 247\n",
      "Batch: 248\n",
      "Batch: 249\n",
      "Batch: 250\n",
      "Batch: 251\n",
      "Batch: 252\n",
      "Batch: 253\n",
      "Batch: 254\n",
      "Batch: 255\n",
      "Batch: 256\n",
      "Batch: 257\n",
      "Batch: 258\n",
      "Batch: 259\n",
      "Batch: 260\n",
      "Batch: 261\n",
      "Batch: 262\n",
      "Batch: 263\n",
      "Batch: 264\n",
      "Batch: 265\n",
      "Batch: 266\n",
      "Batch: 267\n",
      "Batch: 268\n",
      "Batch: 269\n",
      "Batch: 270\n",
      "Batch: 271\n",
      "Batch: 272\n",
      "Batch: 273\n",
      "Batch: 274\n",
      "Batch: 275\n",
      "Batch: 276\n",
      "Batch: 277\n",
      "Batch: 278\n",
      "Batch: 279\n",
      "Batch: 280\n",
      "Batch: 281\n",
      "Batch: 282\n",
      "Batch: 283\n",
      "Batch: 284\n",
      "Batch: 285\n",
      "Batch: 286\n",
      "Batch: 287\n",
      "Batch: 288\n",
      "Batch: 289\n",
      "Batch: 290\n",
      "Batch: 291\n",
      "Batch: 292\n",
      "Batch: 293\n",
      "Batch: 294\n",
      "Batch: 295\n",
      "Batch: 296\n",
      "Batch: 297\n",
      "Batch: 298\n",
      "Batch: 299\n",
      "Batch: 300\n",
      "Batch: 301\n",
      "Batch: 302\n",
      "Batch: 303\n",
      "Batch: 304\n",
      "Batch: 305\n",
      "Batch: 306\n",
      "Batch: 307\n",
      "Batch: 308\n",
      "Batch: 309\n",
      "Batch: 310\n",
      "Batch: 311\n",
      "Batch: 312\n",
      "Batch: 313\n",
      "Batch: 314\n",
      "Batch: 315\n",
      "Batch: 316\n",
      "Batch: 317\n",
      "Batch: 318\n",
      "Batch: 319\n",
      "Batch: 320\n",
      "Batch: 321\n",
      "Batch: 322\n",
      "Batch: 323\n",
      "Batch: 324\n",
      "Batch: 325\n",
      "Batch: 326\n",
      "Batch: 327\n",
      "Batch: 328\n",
      "Batch: 329\n",
      "Batch: 330\n",
      "Batch: 331\n",
      "Batch: 332\n",
      "Batch: 333\n",
      "Batch: 334\n",
      "Batch: 335\n",
      "Hours elapsed:  0.2\n"
     ]
    }
   ],
   "source": [
    "#Predicting batches\n",
    "y_test_pred_nlptown = predict_batches_nlptown(batches_X_test, list(X_test[\"clause_ABSA\"].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87     30212\n",
      "           1       0.16      0.29      0.20      3310\n",
      "\n",
      "    accuracy                           0.78     33522\n",
      "   macro avg       0.54      0.56      0.54     33522\n",
      "weighted avg       0.84      0.78      0.81     33522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating predictions on total test set\n",
    "print(classification_report(y_test[\"sentiment\"], y_test_pred_nlptown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.4: <a class=\"anchor\" id=\"section_1_4\"></a> PyABSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load sentiment classifier from checkpoints/APC_MULTILINGUAL_CHECKPOINT/any_model/fast_lcf_bert_Multilingual_acc_94.72_f1_90.07\n",
      "config: checkpoints/APC_MULTILINGUAL_CHECKPOINT/any_model/fast_lcf_bert_Multilingual_acc_94.72_f1_90.07/fast_lcf_bert.config\n",
      "state_dict: checkpoints/APC_MULTILINGUAL_CHECKPOINT/any_model/fast_lcf_bert_Multilingual_acc_94.72_f1_90.07/fast_lcf_bert.state_dict\n",
      "model: None\n",
      "tokenizer: checkpoints/APC_MULTILINGUAL_CHECKPOINT/any_model/fast_lcf_bert_Multilingual_acc_94.72_f1_90.07/fast_lcf_bert.tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/mdeberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config used in Training:\n",
      "ABSADatasetsVersion:2021.12.06\t-->\tCalling Count:0\n",
      "PyABSAVersion:1.8.2\t-->\tCalling Count:0\n",
      "SRD:3\t-->\tCalling Count:0\n",
      "auto_device:True\t-->\tCalling Count:4116\n",
      "batch_size:16\t-->\tCalling Count:6\n",
      "cache_dataset:True\t-->\tCalling Count:2\n",
      "cross_validate_fold:-1\t-->\tCalling Count:2\n",
      "dataset_file:{'train': ['integrated_datasets/apc_datasets/SemEval/laptop14/Laptops_Train.xml.seg', 'integrated_datasets/apc_datasets/SemEval/restaurant16/restaurant_train.raw', 'integrated_datasets/apc_datasets/SemEval/restaurant16/restaurant_train.raw.augment', 'integrated_datasets/apc_datasets/ACL_Twitter/acl-14-short-data/train.raw', 'integrated_datasets/apc_datasets/MAMS/train.xml.dat', 'integrated_datasets/apc_datasets/Television/Television_Train.xml.seg', 'integrated_datasets/apc_datasets/TShirt/Menstshirt_Train.xml.seg', 'integrated_datasets/apc_datasets/Yelp/yelp.train.txt', 'integrated_datasets/apc_datasets/Chinese/phone/phone.train.txt', 'integrated_datasets/apc_datasets/Chinese/camera/camera.train.txt', 'integrated_datasets/apc_datasets/Chinese/notebook/notebook.train.txt', 'integrated_datasets/apc_datasets/Chinese/car/car.train.txt'], 'test': ['integrated_datasets/apc_datasets/SemEval/laptop14/Laptops_Test_Gold.xml.seg', 'integrated_datasets/apc_datasets/SemEval/restaurant16/restaurant_test.raw', 'integrated_datasets/apc_datasets/ACL_Twitter/acl-14-short-data/test.raw', 'integrated_datasets/apc_datasets/MAMS/test.xml.dat', 'integrated_datasets/apc_datasets/Television/Television_Test_Gold.xml.seg', 'integrated_datasets/apc_datasets/TShirt/Menstshirt_Test_Gold.xml.seg', 'integrated_datasets/apc_datasets/Yelp/yelp.test.txt', 'integrated_datasets/apc_datasets/Chinese/phone/phone.test.txt', 'integrated_datasets/apc_datasets/Chinese/camera/camera.test.txt', 'integrated_datasets/apc_datasets/Chinese/notebook/notebook.test.txt', 'integrated_datasets/apc_datasets/Chinese/car/car.test.txt'], 'valid': ['integrated_datasets/apc_datasets/SemEval/laptop14/Laptops_Valid_Gold.xml.seg', 'integrated_datasets/apc_datasets/SemEval/restaurant16/restaurant_valid.raw', 'integrated_datasets/apc_datasets/MAMS/valid.xml.dat']}\t-->\tCalling Count:4115\n",
      "dataset_item:['Laptop14', 'Restaurant16', 'ACL_Twitter', 'MAMS', 'Television', 'TShirt', 'Yelp', 'Phone', 'Camera', 'Notebook', 'Car']\t-->\tCalling Count:0\n",
      "dataset_name:Multilingual\t-->\tCalling Count:4\n",
      "dca_layer:3\t-->\tCalling Count:0\n",
      "dca_p:1\t-->\tCalling Count:0\n",
      "deep_ensemble:False\t-->\tCalling Count:0\n",
      "device:cuda:0\t-->\tCalling Count:17955\n",
      "device_name:NVIDIA GeForce RTX 3090\t-->\tCalling Count:0\n",
      "dlcf_a:2\t-->\tCalling Count:0\n",
      "dropout:0.5\t-->\tCalling Count:2\n",
      "dynamic_truncate:True\t-->\tCalling Count:0\n",
      "embed_dim:768\t-->\tCalling Count:6\n",
      "ensemble_mode:cat\t-->\tCalling Count:0\n",
      "eta:-1\t-->\tCalling Count:0\n",
      "evaluate_begin:5\t-->\tCalling Count:1644\n",
      "hidden_dim:768\t-->\tCalling Count:0\n",
      "index_to_label:{0: 'Negative', 1: 'Neutral', 2: 'Positive'}\t-->\tCalling Count:0\n",
      "initializer:xavier_uniform_\t-->\tCalling Count:1\n",
      "inputs_cols:['lcf_vec', 'text_bert_indices', 'text_raw_bert_indices']\t-->\tCalling Count:4492\n",
      "l2reg:1e-06\t-->\tCalling Count:1\n",
      "label_to_index:{'Negative': 0, 'Neutral': 1, 'Positive': 2}\t-->\tCalling Count:0\n",
      "lcf:cdw\t-->\tCalling Count:0\n",
      "learning_rate:2e-05\t-->\tCalling Count:1\n",
      "log_step:5\t-->\tCalling Count:4117\n",
      "max_seq_len:80\t-->\tCalling Count:8974\n",
      "max_test_metrics:{'max_apc_test_acc': 0.9471913891691893, 'max_apc_test_f1': 0.9006938075375056, 'max_ate_test_f1': 0}\t-->\tCalling Count:8\n",
      "metrics_of_this_checkpoint:{'acc': 0.9471913891691893, 'f1': 0.9006938075375056}\t-->\tCalling Count:4\n",
      "model:<class 'pyabsa.core.apc.models.fast_lcf_bert.FAST_LCF_BERT'>\t-->\tCalling Count:7\n",
      "model_name:fast_lcf_bert\t-->\tCalling Count:11\n",
      "model_path_to_save:checkpoints\t-->\tCalling Count:6\n",
      "num_epoch:10\t-->\tCalling Count:2\n",
      "optimizer:adam\t-->\tCalling Count:1\n",
      "patience:16407335.924999999\t-->\tCalling Count:4\n",
      "polarities_dim:3\t-->\tCalling Count:6\n",
      "pretrained_bert:microsoft/mdeberta-v3-base\t-->\tCalling Count:8\n",
      "save_mode:1\t-->\tCalling Count:3\n",
      "seed:52\t-->\tCalling Count:7\n",
      "sigma:0.3\t-->\tCalling Count:0\n",
      "similarity_threshold:1\t-->\tCalling Count:0\n",
      "srd_alignment:True\t-->\tCalling Count:0\n",
      "use_bert_spc:True\t-->\tCalling Count:4487\n",
      "use_syntax_based_SRD:False\t-->\tCalling Count:0\n",
      "window:lr\t-->\tCalling Count:0\n",
      "Can not load en_core_web_sm from spacy, try to download it in order to parse syntax tree: \u001b[32m\n",
      "python -m spacy download en_core_web_sm\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Setting classifier\n",
    "classifier = APCCheckpointManager.get_sentiment_classifier(checkpoint = \"multilingual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to get sentiment\n",
    "def get_PYABSA_sentiment(classifier, string):\n",
    "    try:\n",
    "        sentiment_dict = classifier.infer(string, print_result = False)\n",
    "        sentiment = sentiment_dict[\"sentiment\"][0]\n",
    "        confidence = sentiment_dict[\"confidence\"][0]\n",
    "        if confidence < 0.9:\n",
    "            return 0\n",
    "        else:\n",
    "            if sentiment == \"Negative\":\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 17044/33522 [23:36<19:25, 14.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAspectEqualsTextWarning -> <aspect: viola_amherd> equals <text:  viola_amherd >, <polarity: -999>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 20597/33522 [28:32<16:16, 13.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAspectEqualsTextWarning -> <aspect: simonetta_sommaruga> equals <text:  simonetta_sommaruga >, <polarity: -999>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 26875/33522 [37:23<08:07, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAspectEqualsTextWarning -> <aspect: swissmedic> equals <text:  swissmedic >, <polarity: -999>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 31672/33522 [44:13<02:25, 12.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAspectEqualsTextWarning -> <aspect: die_mitte> equals <text:  die_mitte >, <polarity: -999>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 31979/33522 [44:38<01:47, 14.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAspectEqualsTextWarning -> <aspect: stefan_kuster> equals <text:  stefan_kuster >, <polarity: -999>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 32154/33522 [44:53<01:42, 13.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAspectEqualsTextWarning -> <aspect: svp> equals <text:  svp >, <polarity: -999>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 32487/33522 [45:21<01:17, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAspectEqualsTextWarning -> <aspect: bundesrat> equals <text:  bundesrat >, <polarity: -999>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 32671/33522 [45:37<01:05, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAspectEqualsTextWarning -> <aspect: bag> equals <text:  bag >, <polarity: -999>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 33308/33522 [46:31<00:15, 13.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAspectEqualsTextWarning -> <aspect: christoph_berger> equals <text:  christoph_berger >, <polarity: -999>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33522/33522 [46:49<00:00, 11.93it/s]\n"
     ]
    }
   ],
   "source": [
    "#Generating predictions\n",
    "y_test_pred_pyabsa = X_test[\"clause_PYABSA\"].progress_apply(lambda x: get_PYABSA_sentiment(classifier, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     30212\n",
      "           1       0.13      0.06      0.08      3310\n",
      "\n",
      "    accuracy                           0.87     33522\n",
      "   macro avg       0.52      0.51      0.50     33522\n",
      "weighted avg       0.83      0.87      0.85     33522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating predictions on total test set\n",
    "print(classification_report(y_test[\"sentiment\"], y_test_pred_pyabsa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: <a class=\"anchor\" id=\"chapter2\"></a> Fine-tuning of Pre-Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"mdraw/german-news-sentiment-bert\", do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining preprocessing function\n",
    "def preprocess(data, max_length):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT model.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in data:\n",
    "        #Encode sentence\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text = sent,\n",
    "            add_special_tokens = True,\n",
    "            max_length = max_length,\n",
    "            pad_to_max_length = True,\n",
    "            #return_tensors = \"pt\",\n",
    "            return_attention_mask = True)\n",
    "        \n",
    "        #Add outputs to the lists\n",
    "        input_ids.append(encoded_sent.get(\"input_ids\"))\n",
    "        attention_masks.append(encoded_sent.get(\"attention_mask\"))\n",
    "\n",
    "    #Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting maximum length\n",
    "full = np.concatenate([X_train_total[\"clause_ABSA\"].values, X_test[\"clause_ABSA\"].values])\n",
    "encoded = [tokenizer.encode(sent, add_special_tokens = True) for sent in full]\n",
    "max_len = max([len(sent) for sent in encoded])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "train_gold_inputs, train_gold_masks = preprocess(X_train_gold[\"clause_ABSA\"].values, max_len)\n",
    "vali_gold_inputs, vali_gold_masks = preprocess(X_vali_gold[\"clause_ABSA\"].values, max_len)\n",
    "test_gold_inputs, test_gold_masks = preprocess(X_test_gold[\"clause_ABSA\"].values, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting labels to Torch tensors\n",
    "train_gold_labels = torch.tensor(y_train_gold[\"sentiment\"].values)\n",
    "vali_gold_labels = torch.tensor(y_vali_gold[\"sentiment\"].values)\n",
    "test_gold_labels = torch.tensor(y_test_gold[\"sentiment\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DataLoader\n",
    "train_gold_data = TensorDataset(train_gold_inputs, train_gold_masks, train_gold_labels)\n",
    "train_gold_sampler = RandomSampler(train_gold_data)\n",
    "train_gold_dataloader = DataLoader(train_gold_data, sampler = train_gold_sampler, batch_size = 75)\n",
    "\n",
    "vali_gold_data = TensorDataset(vali_gold_inputs, vali_gold_masks, vali_gold_labels)\n",
    "vali_gold_sampler = SequentialSampler(vali_gold_data)\n",
    "vali_gold_dataloader = DataLoader(vali_gold_data, sampler = vali_gold_sampler, batch_size = 75)\n",
    "\n",
    "test_gold_data = TensorDataset(test_gold_inputs, test_gold_masks, test_gold_labels)\n",
    "test_gold_sampler = SequentialSampler(test_gold_data)\n",
    "test_gold_dataloader = DataLoader(test_gold_data, sampler = test_gold_sampler, batch_size = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining BertClassifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze_bert = False):\n",
    "        \"\"\"\n",
    "        @param    bert: BertModel object\n",
    "        @param    classifier: Torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set to False to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        #Specify hidden size of BERT, hidden size of classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        #Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained(\"mdraw/german-news-sentiment-bert\")\n",
    "\n",
    "        #Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, D_out))\n",
    "\n",
    "        #Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        @param    input_ids (torch.Tensor): Input tensor with shape (batch_size, max_length)\n",
    "        @param    attention_mask (torch.Tensor): Tensor that holds attention mask information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): Output tensor with shape (batch_size, num_labels)\n",
    "        \"\"\"\n",
    "        #Feed input to BERT\n",
    "        outputs = self.bert(input_ids = input_ids,\n",
    "                            attention_mask = attention_mask)\n",
    "        \n",
    "        #Extract the last hidden state of the token [CLS] for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        #Feed the last hidden state to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to initialize model\n",
    "def initialize_model(epochs = 5):\n",
    "    #Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert = False)\n",
    "\n",
    "    #Instantiate optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr = 5e-5,\n",
    "                      eps = 1e-8)\n",
    "\n",
    "    #Total number of training steps\n",
    "    total_steps = len(train_gold_dataloader) * epochs\n",
    "\n",
    "    #Instantiate learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps = 0,\n",
    "                                                num_training_steps = total_steps)\n",
    "    \n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to set random seet\n",
    "def set_seed(seed_value = 1):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to train model\n",
    "def train(model, train_dataloader, vali_dataloader, epochs = 5, evaluation = False):\n",
    "    print(\"Start training...\\n\")\n",
    "    \n",
    "    #For each epoch...\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*69)\n",
    "        \n",
    "        #Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        #Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        #Put the model into training mode\n",
    "        model.train()\n",
    "        \n",
    "        #For each batch...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            \n",
    "            #Load batches\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t for t in batch)\n",
    "\n",
    "            #Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            #Perform a forward pass and return logits\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "            \n",
    "            #Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            #Perform a backward pass and return gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            #Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            #Print the loss values and time elapsed for every 5th batch\n",
    "            if (step % 5 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                #Calculate time elapsed\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                #Print results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                #Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        #Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*69)\n",
    "        \n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            #After the completion of each training epoch, measure the model's performance\n",
    "            vali_loss, vali_accuracy = evaluate(model, vali_dataloader)\n",
    "\n",
    "            #Calculate time elapsed\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            #Print results\n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {vali_loss:^10.6f} | {vali_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*69)\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to evaluate model\n",
    "def evaluate(model, vali_dataloader):\n",
    "    #Put the model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #Set tracking variables\n",
    "    vali_accuracy = []\n",
    "    vali_loss = []\n",
    "\n",
    "    #For each batch...\n",
    "    for batch in vali_dataloader:\n",
    "        #Load batches\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t for t in batch)\n",
    "\n",
    "        #Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        #Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        vali_loss.append(loss.item())\n",
    "\n",
    "        #Get predictions\n",
    "        preds = torch.argmax(logits, dim = 1).flatten()\n",
    "\n",
    "        #Compute accuracy score\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        vali_accuracy.append(accuracy)\n",
    "\n",
    "    #Compute the average accuracy and loss over the validation set\n",
    "    vali_loss = np.mean(vali_loss)\n",
    "    vali_accuracy = np.mean(vali_accuracy)\n",
    "\n",
    "    return vali_loss, vali_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mdraw/german-news-sentiment-bert were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#Setting seed and initializing model\n",
    "set_seed(1)\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "---------------------------------------------------------------------\n",
      "   1    |    5    |   0.518090   |     -      |     -     |  166.91  \n",
      "   1    |   10    |   0.365265   |     -      |     -     |  139.48  \n",
      "   1    |   15    |   0.374636   |     -      |     -     |  138.88  \n",
      "   1    |   20    |   0.318443   |     -      |     -     |  139.14  \n",
      "   1    |   25    |   0.348795   |     -      |     -     |  139.69  \n",
      "   1    |   26    |   0.426620   |     -      |     -     |   3.31   \n",
      "---------------------------------------------------------------------\n",
      "   1    |    -    |   0.391513   |  0.325748  |   88.14   |  785.86  \n",
      "---------------------------------------------------------------------\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "---------------------------------------------------------------------\n",
      "   2    |    5    |   0.283663   |     -      |     -     |  152.76  \n",
      "   2    |   10    |   0.267601   |     -      |     -     |  124.58  \n",
      "   2    |   15    |   0.281754   |     -      |     -     |  125.23  \n",
      "   2    |   20    |   0.246758   |     -      |     -     |  125.09  \n",
      "   2    |   25    |   0.250572   |     -      |     -     |  125.53  \n",
      "   2    |   26    |   0.422964   |     -      |     -     |   3.06   \n",
      "---------------------------------------------------------------------\n",
      "   2    |    -    |   0.272532   |  0.336278  |   88.35   |  712.88  \n",
      "---------------------------------------------------------------------\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "---------------------------------------------------------------------\n",
      "   3    |    5    |   0.203649   |     -      |     -     |  150.36  \n",
      "   3    |   10    |   0.187703   |     -      |     -     |  122.10  \n",
      "   3    |   15    |   0.205699   |     -      |     -     |  123.36  \n",
      "   3    |   20    |   0.182950   |     -      |     -     |  122.64  \n",
      "   3    |   25    |   0.173054   |     -      |     -     |  123.23  \n",
      "   3    |   26    |   0.149833   |     -      |     -     |   2.96   \n",
      "---------------------------------------------------------------------\n",
      "   3    |    -    |   0.189584   |  0.342397  |   85.24   |  700.63  \n",
      "---------------------------------------------------------------------\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "train(bert_classifier, train_gold_dataloader, vali_gold_dataloader, epochs = 3, evaluation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to generate predictions\n",
    "def predict(model, test_dataloader):\n",
    "    #Put the model into evaluation mode \n",
    "    model.eval()\n",
    "    \n",
    "    #Set tracking variables\n",
    "    all_logits = []\n",
    "\n",
    "    #For each batch...\n",
    "    for batch in test_dataloader:\n",
    "        #Load batches\n",
    "        b_input_ids, b_attn_mask = tuple(t for t in batch)[:2]\n",
    "\n",
    "        #Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    #Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim = 0)\n",
    "\n",
    "    #Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim = 1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating predictions\n",
    "y_test_gold_pred_finetuned_proba = predict(bert_classifier, test_gold_dataloader)\n",
    "y_test_gold_pred_finetuned = np.where(y_test_gold_pred_finetuned_proba[:, 1] > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92      2123\n",
      "           1       0.37      0.26      0.30       282\n",
      "\n",
      "    accuracy                           0.86      2405\n",
      "   macro avg       0.64      0.60      0.61      2405\n",
      "weighted avg       0.84      0.86      0.85      2405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating predictions on gold test set\n",
    "print(classification_report(y_test_gold[\"sentiment\"], y_test_gold_pred_finetuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: <a class=\"anchor\" id=\"chapter3\"></a> Deep Learning from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1: <a class=\"anchor\" id=\"section_3_1\"></a> Sequential Neutral Network with Self-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing\n",
    "tokenizer = Tokenizer(lower = True, \n",
    "                      split = \" \")\n",
    "tokenizer.fit_on_texts(list(X_train[\"clause_ABSA\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text into integer sequences\n",
    "X_train_total_seq  = tokenizer.texts_to_sequences(X_train_total[\"clause_ABSA\"])\n",
    "X_train_seq  = tokenizer.texts_to_sequences(X_train[\"clause_ABSA\"]) \n",
    "X_vali_seq = tokenizer.texts_to_sequences(X_vali[\"clause_ABSA\"])\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test[\"clause_ABSA\"])\n",
    "\n",
    "X_train_gold_seq = tokenizer.texts_to_sequences(X_train_gold[\"clause_ABSA\"])\n",
    "X_vali_gold_seq = tokenizer.texts_to_sequences(X_vali_gold[\"clause_ABSA\"])\n",
    "X_test_gold_seq = tokenizer.texts_to_sequences(X_test_gold[\"clause_ABSA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting maximum length\n",
    "full = np.concatenate([X_train_total_seq, X_test_seq])\n",
    "max_len = max(full, key = len)\n",
    "max_len = len(max_len)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding sequences\n",
    "X_train_total_seq  = pad_sequences(X_train_total_seq, maxlen = 160)\n",
    "X_train_seq  = pad_sequences(X_train_seq, maxlen = 160) \n",
    "X_vali_seq = pad_sequences(X_vali_seq, maxlen = 160)\n",
    "X_test_seq = pad_sequences(X_test_seq, maxlen = 160)\n",
    "\n",
    "X_train_gold_seq  = pad_sequences(X_train_gold_seq, maxlen = 160)\n",
    "X_vali_gold_seq = pad_sequences(X_vali_gold_seq, maxlen = 160)\n",
    "X_test_gold_seq = pad_sequences(X_test_gold_seq, maxlen = 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59896"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting size of vocabulary\n",
    "size_of_vocabulary = len(tokenizer.word_index) + 1\n",
    "size_of_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating model\n",
    "model = Sequential()\n",
    "model.add(Embedding(size_of_vocabulary, 300, input_length = 160, trainable = True)) \n",
    "model.add(LSTM(128, return_sequences = True, dropout = 0.2))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation = \"relu\")) \n",
    "model.add(Dense(1, activation = \"sigmoid\")) \n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"binary_accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding callbacks\n",
    "#Early stopping monitor\n",
    "es = EarlyStopping(monitor = \"val_loss\", mode = \"min\", verbose = 1, patience = 3)  \n",
    "\n",
    "#Learning rate decay\n",
    "def lr_decay(initial_lr, decay_rate):\n",
    "    def lr_decay_fn(epoch):\n",
    "        return initial_lr * (1 / (1 + decay_rate * epoch))\n",
    "    return lr_decay_fn\n",
    "\n",
    "decay = lr_decay(0.01, 0.1)\n",
    "lr = LearningRateScheduler(decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 160, 300)          17968800  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 160, 128)          219648    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 18,196,769\n",
      "Trainable params: 18,196,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Printing model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3172/3172 [==============================] - 667s 210ms/step - loss: 0.2128 - binary_accuracy: 0.9195 - val_loss: 0.1830 - val_binary_accuracy: 0.9308\n",
      "Epoch 2/5\n",
      "3172/3172 [==============================] - 667s 210ms/step - loss: 0.1393 - binary_accuracy: 0.9455 - val_loss: 0.1758 - val_binary_accuracy: 0.9325\n",
      "Epoch 3/5\n",
      "3172/3172 [==============================] - 662s 209ms/step - loss: 0.1095 - binary_accuracy: 0.9585 - val_loss: 0.1833 - val_binary_accuracy: 0.9322\n",
      "Epoch 4/5\n",
      "3172/3172 [==============================] - 692s 218ms/step - loss: 0.0923 - binary_accuracy: 0.9649 - val_loss: 0.1922 - val_binary_accuracy: 0.9328\n",
      "Epoch 5/5\n",
      "3172/3172 [==============================] - 706s 223ms/step - loss: 0.0792 - binary_accuracy: 0.9699 - val_loss: 0.2001 - val_binary_accuracy: 0.9322\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6c38b72190>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting model\n",
    "model.fit(x = np.array(X_train_seq), \n",
    "          y = np.array(y_train[\"sentiment\"]).astype(\"float32\").reshape((-1,1)),\n",
    "          validation_data = ((np.array(X_vali_seq), \n",
    "                              np.array(y_vali[\"sentiment\"]).astype(\"float32\").reshape((-1,1)))),\n",
    "          epochs = 5, \n",
    "          callbacks = [es, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing loss development\n",
    "#Preparing data\n",
    "losses = pd.DataFrame(model.history.history)\n",
    "\n",
    "#Creating plot architecture\n",
    "fig, ax = plt.subplots(figsize = (12, 6), ncols = 1, nrows = 1)\n",
    "sns.despine(top = True)\n",
    "fig.suptitle(\"Loss development of neural network\",\n",
    "             color = \"#696969\",\n",
    "             weight = \"bold\",\n",
    "             size = 12)\n",
    "\n",
    "#Plotting\n",
    "plot = sns.lineplot(x = range(0, len(losses)),\n",
    "                    y = losses[\"val_loss\"],\n",
    "                    ax = ax,\n",
    "                    color = \"#add1de\")\n",
    "plot = sns.lineplot(x = range(0, len(losses)),\n",
    "                    y = losses[\"loss\"],\n",
    "                    ax = ax,\n",
    "                    color = \"#bfdead\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "\n",
    "#Setting parameters\n",
    "ax.set_xlim([0, 4])\n",
    "ax.set_ylim([0, 0.5])\n",
    "\n",
    "#Setting legend\n",
    "train_loss = mpatches.Patch(color = \"#bfdead\", label = \"Training loss\")\n",
    "vali_loss = mpatches.Patch(color = \"#add1de\", label = \"Validation loss\")\n",
    "legend = ax.legend(handles = [train_loss, vali_loss], \n",
    "                        loc = \"upper right\", \n",
    "                        bbox_to_anchor = (0.9,1.02,0.35,0),\n",
    "                        frameon = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating predictions\n",
    "y_test_pred_embedding = model.predict(np.array(X_test_seq))\n",
    "y_test_pred_embedding = pd.Series(np.where(y_test_pred_embedding > 0.5, 1, 0).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96     30212\n",
      "           1       0.68      0.55      0.61      3310\n",
      "\n",
      "    accuracy                           0.93     33522\n",
      "   macro avg       0.82      0.76      0.78     33522\n",
      "weighted avg       0.92      0.93      0.93     33522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating predictions on total test set\n",
    "print(classification_report(y_test[\"sentiment\"], y_test_pred_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating predictions\n",
    "y_test_gold_pred_embedding = model.predict(np.array(X_test_gold_seq))\n",
    "y_test_gold_pred_embedding = pd.Series(np.where(y_test_gold_pred_embedding > 0.5, 1, 0).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92      2123\n",
      "           1       0.37      0.24      0.30       282\n",
      "\n",
      "    accuracy                           0.86      2405\n",
      "   macro avg       0.64      0.60      0.61      2405\n",
      "weighted avg       0.84      0.86      0.85      2405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating predictions on gold test set\n",
    "print(classification_report(y_test_gold[\"sentiment\"], y_test_gold_pred_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: <a class=\"anchor\" id=\"chapter4\"></a> Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text into integer sequence\n",
    "X_seq  = tokenizer.texts_to_sequences(X[\"clause_ABSA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding sequence\n",
    "X_seq  = pad_sequences(X_seq, maxlen = 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating predictions\n",
    "y_pred_embedding = model.predict(np.array(X_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining dataframe\n",
    "final_df = df_unique.copy()\n",
    "final_df[\"prediction\"] = y_pred_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming columns\n",
    "final_df.rename(columns = {\"sentiment\": \"snorkel_sentiment\", \n",
    "                           \"prediction\": \"sentiment\"}, \n",
    "                inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "final_df.drop([\"index\", \"stratification\", \"sentence_ABSA_subclause\", \"sentence_ABSA_rel_keywords\", \"entity_keyword\"], \n",
    "              axis = 1, \n",
    "              inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to CSV\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "final_df.to_csv(\"Outputs/Articles/DL/dl.csv\")\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
