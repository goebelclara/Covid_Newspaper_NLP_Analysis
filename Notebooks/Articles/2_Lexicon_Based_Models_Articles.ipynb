{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Chapter 1](#chapter1): Data Preprocessing\n",
    "* [Chapter 2](#chapter2): Sentiment Analysis\n",
    "    * [Section 2.1](#section_2_1): Defining Functions\n",
    "    * [Section 2.2](#section_2_2): TextBlob\n",
    "    * [Section 2.3](#section_2_3): SentiWS\n",
    "* [Chapter 3](#chapter3): Smoothed Sentiment Analysis Over Time\n",
    "    * [Section 3.1](#section_3_1): Defining Functions\n",
    "    * [Section 3.2](#section_3_2): TextBlob\n",
    "    * [Section 3.3](#section_3_3): SentiWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: <a class=\"anchor\" id=\"chapter1\"></a> Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Performing required installations\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#!python3 -m spacy download de_core_news_md\n",
    "\n",
    "#pip install textblob_de\n",
    "\n",
    "#pip install spacy\n",
    "#pip install spacy_sentiws\n",
    "#pip install spacy-transformers\n",
    "\n",
    "#pip install polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "#Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "#Sentiment analysis\n",
    "from textblob_de import TextBlobDE\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string as st\n",
    "import spacy\n",
    "import spacy.cli\n",
    "from spacy_sentiws import spaCySentiWS\n",
    "\n",
    "#Timeseries and date handling\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt \n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft, fftshift\n",
    "from math import factorial\n",
    "from astropy.convolution import convolve, Box1DKernel, Gaussian1DKernel, Trapezoid1DKernel\n",
    "\n",
    "#Stats\n",
    "from statistics import mean\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats\n",
    "import math\n",
    "\n",
    "#Other\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suppressing warnings\n",
    "warnings.simplefilter(action = \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in CSVs\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df_cleaned_parsed = pd.read_csv(\"Data/Articles/cleaned_parsed_data.csv\", index_col = 0, parse_dates = [\"pubtime\", \"pubday\", \"pubmonth\"])\n",
    "df = pd.read_csv(\"Data/Articles/cleaned_parsed_preprocessed_data.csv\", index_col = 0, parse_dates = [\"pubtime\", \"pubday\", \"pubmonth\"])\n",
    "\n",
    "df_textblob = pd.read_csv(\"Outputs/Articles/Lexicon/textblob.csv\", index_col = 0, parse_dates = [\"pubtime\", \"pubday\", \"pubmonth\"])\n",
    "df_textblob_agg = pd.read_csv(\"Outputs/Articles/Lexicon/textblob_agg.csv\", index_col = 0)\n",
    "df_textblob_timeseries = pd.read_csv(\"Outputs/Articles/Lexicon/textblob_timeseries.csv\", index_col = 0)\n",
    "df_textblob_timeseries.index = pd.to_datetime(df_textblob_timeseries.index)\n",
    "\n",
    "df_sentiws = pd.read_csv(\"Outputs/Articles/Lexicon/sentiws.csv\", index_col = 0, parse_dates = [\"pubtime\", \"pubday\", \"pubmonth\"])\n",
    "df_sentiws_agg = pd.read_csv(\"Outputs/Articles/Lexicon/sentiws_agg.csv\", index_col = 0)\n",
    "df_sentiws_timeseries = pd.read_csv(\"Outputs/Articles/Lexicon/sentiws_timeseries.csv\", index_col = 0)\n",
    "df_sentiws_timeseries.index = pd.to_datetime(df_sentiws_timeseries.index)\n",
    "\n",
    "df_entities = pd.read_csv(\"Inputs/Articles/entities.csv\")\n",
    "df_key_media = pd.read_csv(\"Inputs/Articles/key_media.csv\", index_col = 0)\n",
    "df_key_entities = pd.read_csv(\"Inputs/Articles/key_entities.csv\", index_col = 0)\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ueli_Maurer',\n",
       " 'Guy_Parmelin',\n",
       " 'Simonetta_Sommaruga',\n",
       " 'Alain_Berset',\n",
       " 'Bundesrat',\n",
       " 'Tanja_Stadler',\n",
       " 'Martin_Ackermann',\n",
       " 'Taskforce',\n",
       " 'Christoph_Berger',\n",
       " 'EKIF',\n",
       " 'Patrick_Mathys',\n",
       " 'Marcel_Salathe',\n",
       " 'Daniel_Koch',\n",
       " 'BAG',\n",
       " 'Swissmedic',\n",
       " 'Lukas_Engelberger',\n",
       " 'GDK',\n",
       " 'SVP',\n",
       " 'SP',\n",
       " 'FDP',\n",
       " 'Die_Mitte',\n",
       " 'Die_Gruene',\n",
       " 'Befuerworter',\n",
       " 'Gegner',\n",
       " 'Skeptiker',\n",
       " 'Kritiker',\n",
       " 'Opposition',\n",
       " 'Demonstranten']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting entities\n",
    "entities = list(df_entities[df_entities[\"selection\"] == 1][\"designed_entity\"].unique())\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NZZ',\n",
       " 'Zuercher_Unterlaender',\n",
       " 'Blick',\n",
       " 'Zuerichsee_Zeitung',\n",
       " 'Berner_Zeitung',\n",
       " 'Solothurner_Zeitung',\n",
       " 'SRF',\n",
       " 'Berner_Oberlaender',\n",
       " 'Tages_Anzeiger',\n",
       " 'Limmattaler_Zeitung',\n",
       " 'Langenthaler_Tagblatt',\n",
       " 'Cash',\n",
       " 'Grenchner_Tagblatt',\n",
       " 'BZ_Basel',\n",
       " 'Handelszeitung',\n",
       " 'Luzerner_Zeitung',\n",
       " 'Landbote',\n",
       " 'Werdenberger_&_Obertoggenburger',\n",
       " 'Aargauer_Zeitung',\n",
       " '20_Minuten',\n",
       " 'Der_Bund',\n",
       " 'Basler_Zeitung',\n",
       " 'Nidwaldner_Zeitung',\n",
       " 'St._Galler_Tagblatt',\n",
       " 'Thuner_Tagblatt',\n",
       " 'Zofinger_Tagblatt',\n",
       " 'Badener_Tagblatt',\n",
       " 'Thurgauer_Zeitung',\n",
       " 'Schweizer_Illustrierte',\n",
       " 'Zuger_Zeitung',\n",
       " 'Finanz_und_Wirtschaft',\n",
       " 'Urner_Zeitung',\n",
       " 'Die_Wochenzeitung',\n",
       " 'Swissinfo',\n",
       " 'Oltner_Tagblatt',\n",
       " 'Obwaldner_Zeitung',\n",
       " 'Appenzeller_Zeitung',\n",
       " 'Das_Magazin',\n",
       " 'Beobachter',\n",
       " 'Bilanz',\n",
       " 'Toggenburger_Tagblatt',\n",
       " 'Thalwiler_Anzeiger',\n",
       " 'Zuger_Presse',\n",
       " 'TV_Star',\n",
       " 'Schweizer_Familie',\n",
       " 'Zugerbieter',\n",
       " 'Glueckspost',\n",
       " 'Tele']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting media\n",
    "media = list(df_cleaned_parsed[\"medium_name\"].unique())\n",
    "media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating nlp\n",
    "nlp = spacy.load(\"de_core_news_md\", disable = [\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linking to SentiWS\n",
    "sentiws = spaCySentiWS(sentiws_path = \"Resources/SentiWS/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Defining function to lemmatize tokens\n",
    "def lemmatize(tokens):\n",
    "    spacy_tokens = [nlp(token) for token in tokens]\n",
    "    lemmas = [spacy_token[0].lemma_ for spacy_token in spacy_tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Defining preprocessing function\n",
    "def preprocess(df):\n",
    "    df = df_cleaned_parsed.copy()\n",
    "    \n",
    "    #Creating new columns\n",
    "    df[\"clause_tokens\"] = df[\"clause_ABSA\"]\n",
    "    df[\"passage_tokens\"] = df[\"passage_ABSA\"]\n",
    "        \n",
    "    #Setting columns\n",
    "    columns = [\"clause_tokens\", \n",
    "               \"passage_tokens\"]\n",
    "      \n",
    "    #Tokenizing\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: x.split())\n",
    "        \n",
    "    #Removing stopwords\n",
    "    stopword = set(stopwords.words(\"german\"))\n",
    "    \n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: [token for token in x if token not in stopword])\n",
    "        \n",
    "    #Removing punctuation\n",
    "    punctuation = list(st.punctuation)\n",
    "    \n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: [token for token in x if token not in punctuation])\n",
    "        \n",
    "    #Removing [NEG_ENT] tokens\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: [token for token in x if token != \"[NEG_ENT]\"])\n",
    "    \n",
    "    #Lemmatizing on sentence- and passage-level\n",
    "    #df[\"clause_tokens_lemmatized\"] = df[\"clause_tokens\"].progress_apply(lambda x: lemmatize(x))\n",
    "    #df[\"passage_tokens_lemmatized\"] = df[\"passage_tokens\"].progress_apply(lambda x: lemmatize(x))\n",
    "    \n",
    "    #Turning token list into token string\n",
    "    columns = [\"clause_tokens\", \n",
    "               \"passage_tokens\", \n",
    "               #\"clause_tokens_lemmatized\", \"passage_tokens_lemmatized\"\n",
    "              ]\n",
    "    new_columns = [\"clause\", \n",
    "                   \"passage\", \n",
    "                   #\"clause_lemmatized\", \"passage_lemmatized\"\n",
    "                  ]\n",
    "\n",
    "    for column, new_column in zip(columns, new_columns):\n",
    "        df[new_column] = df[column].apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "df = preprocess(df_cleaned_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to CSV\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df.to_csv(\"Data/Articles/cleaned_parsed_preprocessed_data.csv\")\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: <a class=\"anchor\" id=\"chapter2\"></a> Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1: <a class=\"anchor\" id=\"section_2_1\"></a> Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Defining function to calculate sentiment and subjectivity for TextBlob\n",
    "def calculate_sentiment_subjectivity_textblob(df, sentiment_col_name, subjectivity_col_name, text_col):\n",
    "    df[sentiment_col_name] = df[text_col].progress_apply(lambda x: TextBlobDE(x).polarity)\n",
    "    df[subjectivity_col_name] = df[text_col].progress_apply(lambda x: TextBlobDE(x).subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Defining function to bin sentiment and subjectivity for TextBlob\n",
    "def bin_sentiment_subjectivity_textblob(df, bins, sentiment_bin_col_name, subjectivity_bin_col_name, sentiment_col_name, subjectivity_col_name):\n",
    "    df[sentiment_bin_col_name] = pd.cut(df[sentiment_col_name], bins, labels = [-1,0,1])\n",
    "    df[sentiment_bin_col_name] = df[sentiment_bin_col_name].astype(\"int64\")\n",
    "    df[subjectivity_bin_col_name] = pd.cut(df[subjectivity_col_name], bins, labels = [-1,0,1])\n",
    "    df[subjectivity_bin_col_name] = df[subjectivity_bin_col_name].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to run TextBlob sentiment analyis\n",
    "def sa_textblob(df, bins):\n",
    "    #Calculating sentiment and subjectivity scores on sentence- and passage- level\n",
    "    calculate_sentiment_subjectivity_textblob(df, \"clause_sentiment_score\", \"clause_subjectivity_score\", \"clause\")\n",
    "    calculate_sentiment_subjectivity_textblob(df, \"passage_sentiment_score\", \"passage_subjectivity_score\", \"passage\")\n",
    "    \n",
    "    #Binning sentiment and subjectivity scores\n",
    "    bin_sentiment_subjectivity_textblob(df, bins, \"clause_sentiment\", \"clause_subjectivity\", \"clause_sentiment_score\", \"clause_subjectivity_score\")\n",
    "    bin_sentiment_subjectivity_textblob(df, bins, \"passage_sentiment\", \"passage_subjectivity\", \"passage_sentiment_score\", \"passage_subjectivity_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to calculate sentiment and subjectivity for SentiWS\n",
    "def calculate_sentiment_sentiws(df, sentiment_col_name, text_col):\n",
    "    def get_score(string):\n",
    "        try:\n",
    "            doc = nlp(string)\n",
    "            score_list = [0 if token._.sentiws == None else float(token._.sentiws) for token in doc]\n",
    "            polarity_list = [1 if score > 0 else -1 for score in score_list if score != 0]\n",
    "            polarity = sum(polarity_list)\n",
    "            sentiment = np.where(polarity > 0, 1, np.where(polarity == 0, 0, -1)).flatten()[0]\n",
    "            return sentiment\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    df[sentiment_col_name] = df[text_col].progress_apply(lambda x: get_score(x))\n",
    "    df[sentiment_col_name] = df[sentiment_col_name].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to run SentiWS sentiment analyis\n",
    "def sa_sentiws(df):\n",
    "    #Calculating sentiment scores on sentence-level\n",
    "    calculate_sentiment_sentiws(df, \"clause_sentiment\", \"clause\")\n",
    "\n",
    "    #Calculating sentiment scores on passage-level\n",
    "    calculate_sentiment_sentiws(df, \"passage_sentiment\", \"passage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: <a class=\"anchor\" id=\"section_2_2\"></a> TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating copy of dataframe\n",
    "df_textblob = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 268001/268001 [11:53<00:00, 375.52it/s]\n",
      "100%|██████████████████████████████████| 268001/268001 [15:49<00:00, 282.40it/s]\n",
      "100%|██████████████████████████████████| 268001/268001 [35:46<00:00, 124.88it/s]\n",
      "100%|██████████████████████████████████| 268001/268001 [28:38<00:00, 155.92it/s]\n"
     ]
    }
   ],
   "source": [
    "#Running TextBlob sentiment analysis\n",
    "sa_textblob(df_textblob, [-1.1,-0.5,0.5,1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Saving to CSV\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df_textblob.to_csv(\"Outputs/Articles/Lexicon/textblob.csv\")\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3: <a class=\"anchor\" id=\"section_2_3\"></a> SentiWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating copy of dataframe\n",
    "df_sentiws = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 268001/268001 [08:50<00:00, 504.93it/s]\n",
      "100%|██████████████████████████████████| 268001/268001 [13:52<00:00, 321.87it/s]\n"
     ]
    }
   ],
   "source": [
    "#Running SentiWS sentiment analysis\n",
    "sa_sentiws(df_sentiws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to CSV\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df_sentiws.to_csv(\"Outputs/Articles/Lexicon/sentiws.csv\")\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: <a class=\"anchor\" id=\"chapter3\"></a> Smoothed Sentiment Analysis Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1: <a class=\"anchor\" id=\"section_3_1\"></a> Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to calculate moving average sentiment of given entity and medium\n",
    "def calculate_moving_avg_std(df, entity, medium, window_size, sentiment_col):\n",
    "    \"\"\"\n",
    "    This function calculates the moving average sentiment for a given entity and/or newspaper.\n",
    "    If the user does not want to filter the sentiment by entity and/or newspaper, the parameters \n",
    "    should be set to False. If the user does not want to return a specific sentiment column,\n",
    "    the parameter should be set to False.\n",
    "    \"\"\"\n",
    "    #Filtering dataframe\n",
    "    if entity == False:\n",
    "        if medium == False:\n",
    "            df_filtered = df\n",
    "        else:\n",
    "            df_filtered = df[df[\"medium_name\"] == medium]\n",
    "    else:\n",
    "        if medium == False:\n",
    "            df_filtered = df[df[\"entity_name\"] == entity]\n",
    "        else:\n",
    "            df_filtered = df[(df[\"medium_name\"] == medium) & (df[\"entity_name\"] == entity)]\n",
    "    \n",
    "    #Creating windows\n",
    "    windows = df_filtered.groupby(\"pubday\").mean().rolling(window = window_size)\n",
    "       \n",
    "    #Calculating average and standard deviation per window\n",
    "    moving_avgs = windows.mean().iloc[window_size-1:,:]\n",
    "    moving_stds = windows.std().iloc[window_size-1:,:]\n",
    "    df_moving = moving_avgs.join(moving_stds, lsuffix = \"_avg\", rsuffix = \"_std\") \n",
    "    \n",
    "    #Calculating sample size\n",
    "    sample_size = df_filtered.groupby(\"pubday\").size()\n",
    "    sample_size.name = \"sample_size\"\n",
    "    df_moving = df_moving.join(sample_size)\n",
    "    \n",
    "    #Dropping ID\n",
    "    df_moving.drop([\"id_avg\", \"id_std\"], axis = 1, inplace = True)\n",
    "    \n",
    "    #Returning information\n",
    "    if sentiment_col == False:\n",
    "        return df_moving\n",
    "    else:\n",
    "        return df_moving[sentiment_col+\"_avg\"], df_moving[sentiment_col+\"_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to calculate Savitzky Golay smoothed sentiment of given entity and medium\n",
    "def calculate_savitzky_golay_smoothed_avg_std(df, entity, medium, interpolation_method, window_size, order, sentiment_col):\n",
    "    \"\"\"\n",
    "    This function calculates the Savitzky Golay smoothed sentiment for a given entity and/or \n",
    "    newspaper. If the user does not want to filter the sentiment by entity and/or newspaper, the \n",
    "    parameters should be set to False. If the user does not want to return a specific sentiment \n",
    "    column, the parameter should be set to False.\n",
    "    \"\"\"\n",
    "    #Filtering dataframe\n",
    "    if entity == False:\n",
    "        if medium == False:\n",
    "            df_filtered = df\n",
    "        else:\n",
    "            df_filtered = df[df[\"medium_name\"] == medium]\n",
    "    else:\n",
    "        if medium == False:\n",
    "            df_filtered = df[df[\"entity_name\"] == entity]\n",
    "        else:\n",
    "            df_filtered = df[(df[\"medium_name\"] == medium) & (df[\"entity_name\"] == entity)]\n",
    "    \n",
    "    #Setting indeces\n",
    "    indeces = pd.period_range(min(df_filtered[\"pubday\"]), max(df_filtered[\"pubday\"]))\n",
    "    indeces = indeces.to_timestamp()\n",
    "    \n",
    "    #Taking daily average, standard deviation, and sample size\n",
    "    avg = df_filtered.groupby(\"pubday\").mean()\n",
    "    avg.columns = [x + \"_avg\" for x in avg.columns]\n",
    "    std = df_filtered.groupby(\"pubday\").std()\n",
    "    std.columns = [x + \"_std\" for x in std.columns]\n",
    "    sample_size = df_filtered.groupby(\"pubday\").size()\n",
    "    sample_size.name = \"sample_size\"\n",
    "    \n",
    "    #Creating timeseries with interpolation\n",
    "    timeseries = pd.DataFrame(index = indeces)\n",
    "    timeseries = timeseries.join(avg).join(std)\n",
    "    timeseries = timeseries.interpolate(method = interpolation_method)\n",
    "    timeseries = timeseries.dropna()\n",
    "    \n",
    "    #Creating smoothed dataframe\n",
    "    df_smoothed = pd.DataFrame(index = timeseries.index)\n",
    "    \n",
    "    #Performing Savitzky Golay smoothing\n",
    "    for col in list(avg.columns) + list(std.columns):\n",
    "        smoothed_values = signal.savgol_filter(timeseries[col], window_size, order)\n",
    "        df_smoothed[col] = smoothed_values\n",
    "    \n",
    "    #Adding sample size\n",
    "    df_smoothed = df_smoothed.join(sample_size).fillna(0)\n",
    "    \n",
    "    #Dropping ID\n",
    "    df_smoothed.drop([\"id_avg\", \"id_std\"], axis = 1, inplace = True)\n",
    "    \n",
    "    #Returning information\n",
    "    if sentiment_col == False:\n",
    "        return df_smoothed\n",
    "    else:\n",
    "        return df_smoothed[sentiment_col+\"_avg\"], df_smoothed[sentiment_col+\"_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to calculate kernel smoothed sentiment of given entity and medium\n",
    "def calculate_kernel_smoothed_avg_std(df, entity, medium, interpolation_method, kernel, width, sentiment_col):\n",
    "    \"\"\"\n",
    "    This function calculates the smoothed sentiment via a selected kernel for a given entity and/or \n",
    "    newspaper. If the user does not want to filter the sentiment by entity and/or newspaper, the \n",
    "    parameters should be set to False. If the user does not want to return a specific sentiment \n",
    "    column, the parameter should be set to False.\n",
    "    \"\"\"\n",
    "    #Filtering dataframe\n",
    "    if entity == False:\n",
    "        if medium == False:\n",
    "            df_filtered = df\n",
    "        else:\n",
    "            df_filtered = df[df[\"medium_name\"] == medium]\n",
    "    else:\n",
    "        if medium == False:\n",
    "            df_filtered = df[df[\"entity_name\"] == entity]\n",
    "        else:\n",
    "            df_filtered = df[(df[\"medium_name\"] == medium) & (df[\"entity_name\"] == entity)]\n",
    "    \n",
    "    #Setting indeces\n",
    "    indeces = pd.period_range(min(df_filtered[\"pubday\"]), max(df_filtered[\"pubday\"]))\n",
    "    indeces = indeces.to_timestamp()\n",
    "    \n",
    "    #Taking daily average, standard deviation, and sample size\n",
    "    avg = df_filtered.groupby(\"pubday\").mean()\n",
    "    avg.columns = [x + \"_avg\" for x in avg.columns]\n",
    "    std = df_filtered.groupby(\"pubday\").std()\n",
    "    std.columns = [x + \"_std\" for x in std.columns]\n",
    "    sample_size = df_filtered.groupby(\"pubday\").size()\n",
    "    sample_size.name = \"sample_size\"\n",
    "    \n",
    "    #Creating timeseries with interpolation\n",
    "    timeseries = pd.DataFrame(index = indeces)\n",
    "    timeseries = timeseries.join(avg).join(std)\n",
    "    timeseries = timeseries.interpolate(method = interpolation_method)\n",
    "    timeseries = timeseries.dropna()\n",
    "    \n",
    "    #Creating smoothed dataframe\n",
    "    df_smoothed = pd.DataFrame(index = timeseries.index)\n",
    "    \n",
    "    #Calculating convolution \n",
    "    for col in list(avg.columns) + list(std.columns):\n",
    "        smoothed_values = convolve(timeseries[col], kernel(width))\n",
    "        df_smoothed[col] = smoothed_values\n",
    "    \n",
    "    #Adding sample size\n",
    "    df_smoothed = df_smoothed.join(sample_size).fillna(0)\n",
    "    \n",
    "    #Dropping ID\n",
    "    df_smoothed.drop([\"id_avg\", \"id_std\"], axis = 1, inplace = True)\n",
    "    \n",
    "    #Returning information\n",
    "    if sentiment_col == False:\n",
    "        return df_smoothed\n",
    "    else:\n",
    "        return df_smoothed[sentiment_col+\"_avg\"], df_smoothed[sentiment_col+\"_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to calculate triangle smoothed sentiment of given entity and medium\n",
    "def calculate_triangle_smoothed_avg_std(df, entity, medium, interpolation_method, degree, sentiment_col):\n",
    "    \"\"\"\n",
    "    This function calculates the triangle smoothed sentiment for a given entity and/or \n",
    "    newspaper. If the user does not want to filter the sentiment by entity and/or newspaper, the \n",
    "    parameters should be set to False. If the user does not want to return a specific sentiment \n",
    "    column, the parameter should be set to False.\n",
    "    \"\"\"\n",
    "    #Filtering dataframe\n",
    "    if entity == False:\n",
    "        if medium == False:\n",
    "            df_filtered = df\n",
    "        else:\n",
    "            df_filtered = df[df[\"medium_name\"] == medium]\n",
    "    else:\n",
    "        if medium == False:\n",
    "            df_filtered = df[df[\"entity_name\"] == entity]\n",
    "        else:\n",
    "            df_filtered = df[(df[\"medium_name\"] == medium) & (df[\"entity_name\"] == entity)]\n",
    "    \n",
    "    #Defining triangle average\n",
    "    def triangle_smoothe(data, degree):\n",
    "        triangle = np.concatenate((np.arange(degree + 1), np.arange(degree)[::-1]))\n",
    "        \n",
    "        smoothed = []\n",
    "        for i in range(degree, len(data) - degree * 2):\n",
    "            point = data[i:i + len(triangle)] * triangle\n",
    "            smoothed.append(np.sum(point) / np.sum(triangle))\n",
    "\n",
    "        #Handle boundaries\n",
    "        smoothed = [smoothed[0]]*int(degree + degree/2) + smoothed\n",
    "        while len(smoothed) < len(data):\n",
    "            smoothed.append(smoothed[-1])\n",
    "\n",
    "        return smoothed\n",
    "\n",
    "    #Setting indeces\n",
    "    indeces = pd.period_range(min(df_filtered[\"pubday\"]), max(df_filtered[\"pubday\"]))\n",
    "    indeces = indeces.to_timestamp()\n",
    "    \n",
    "    #Taking daily average, standard deviation, and sample size\n",
    "    avg = df_filtered.groupby(\"pubday\").mean()\n",
    "    avg.columns = [x + \"_avg\" for x in avg.columns]\n",
    "    std = df_filtered.groupby(\"pubday\").std()\n",
    "    std.columns = [x + \"_std\" for x in std.columns]\n",
    "    sample_size = df_filtered.groupby(\"pubday\").size()\n",
    "    sample_size.name = \"sample_size\"\n",
    "    \n",
    "    #Creating timeseries with interpolation\n",
    "    timeseries = pd.DataFrame(index = indeces)\n",
    "    timeseries = timeseries.join(avg).join(std)\n",
    "    timeseries = timeseries.interpolate(method = interpolation_method)\n",
    "    timeseries = timeseries.dropna()\n",
    "    \n",
    "    #Creating smoothed dataframe\n",
    "    df_smoothed = pd.DataFrame(index = timeseries.index)\n",
    "    \n",
    "    #Performing triangle smoothing\n",
    "    for col in list(avg.columns) + list(std.columns):\n",
    "        smoothed_values = triangle_smoothe(timeseries[col], degree)\n",
    "        df_smoothed[col] = smoothed_values\n",
    "    \n",
    "    #Adding sample size\n",
    "    df_smoothed = df_smoothed.join(sample_size).fillna(0)\n",
    "    \n",
    "    #Dropping ID\n",
    "    df_smoothed.drop([\"id_avg\", \"id_std\"], axis = 1, inplace = True)\n",
    "    \n",
    "    #Returning information\n",
    "    if sentiment_col == False:\n",
    "        return df_smoothed\n",
    "    else:\n",
    "        return df_smoothed[sentiment_col+\"_avg\"], df_smoothed[sentiment_col+\"_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to calculate LOWESS smoothed sentiment of given entity and medium\n",
    "def calculate_lowess_smoothed_avg_std(df, entity, medium, interpolation_method, fraction, sentiment_col):\n",
    "    \"\"\"\n",
    "    This function calculates the LOWESS smoothed sentiment for a given entity and/or \n",
    "    newspaper. If the user does not want to filter the sentiment by entity and/or newspaper, the \n",
    "    parameters should be set to False. If the user does not want to return a specific sentiment \n",
    "    column, the parameter should be set to False.\n",
    "    \"\"\"\n",
    "    #Filtering dataframe\n",
    "    if entity == False:\n",
    "        if medium == False:\n",
    "            df_filtered = df\n",
    "        else:\n",
    "            df_filtered = df[df[\"medium_name\"] == medium]\n",
    "    else:\n",
    "        if medium == False:\n",
    "            df_filtered = df[df[\"entity_name\"] == entity]\n",
    "        else:\n",
    "            df_filtered = df[(df[\"medium_name\"] == medium) & (df[\"entity_name\"] == entity)]\n",
    "\n",
    "    #Setting indeces\n",
    "    indeces = pd.period_range(min(df_filtered[\"pubday\"]), max(df_filtered[\"pubday\"]))\n",
    "    indeces = indeces.to_timestamp()\n",
    "    \n",
    "    #Taking daily average, standard deviation, and sample size\n",
    "    avg = df_filtered.groupby(\"pubday\").mean()\n",
    "    avg.columns = [x + \"_avg\" for x in avg.columns]\n",
    "    std = df_filtered.groupby(\"pubday\").std()\n",
    "    std.columns = [x + \"_std\" for x in std.columns]\n",
    "    sample_size = df_filtered.groupby(\"pubday\").size()\n",
    "    sample_size.name = \"sample_size\"\n",
    "    \n",
    "    #Creating timeseries with interpolation\n",
    "    timeseries = pd.DataFrame(index = indeces)\n",
    "    timeseries = timeseries.join(avg).join(std)\n",
    "    timeseries = timeseries.interpolate(method = interpolation_method)\n",
    "    timeseries = timeseries.dropna()\n",
    "    \n",
    "    #Creating smoothed dataframe\n",
    "    df_smoothed = pd.DataFrame(index = timeseries.index)\n",
    "    \n",
    "    #Fitting LOWESS\n",
    "    lowess = sm.nonparametric.lowess\n",
    "    for col in list(avg.columns) + list(std.columns):\n",
    "        smoothed_values = lowess(timeseries[col].values, timeseries[col].index, is_sorted = True, frac = fraction)\n",
    "        smoothed_values = smoothed_values[:,1]\n",
    "        df_smoothed[col] = smoothed_values\n",
    "    \n",
    "    #Adding sample size\n",
    "    df_smoothed = df_smoothed.join(sample_size).fillna(0)\n",
    "    \n",
    "    #Dropping ID\n",
    "    df_smoothed.drop([\"id_avg\", \"id_std\"], axis = 1, inplace = True)\n",
    "    \n",
    "    #Returning information\n",
    "    if sentiment_col == False:\n",
    "        return df_smoothed\n",
    "    else:\n",
    "        return df_smoothed[sentiment_col+\"_avg\"], df_smoothed[sentiment_col+\"_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to calculate exponentially smoothed sentiment of given entity and medium\n",
    "def calculate_exp_smoothed_avg_std(df, entity, medium, exp_smoothing_method, interpolation_method, smoothing_level, sentiment_col):\n",
    "    \"\"\"\n",
    "    This function calculates the exponentially smoothed average sentiment for a given entity and/or \n",
    "    newspaper. If the user does not want to filter the sentiment by entity and/or newspaper, the \n",
    "    parameters should be set to False. If the user does not want to return a specific sentiment \n",
    "    column, the parameter should be set to False.\n",
    "    \"\"\"\n",
    "    #Filtering dataframe\n",
    "    if entity == False:\n",
    "        if medium == False:\n",
    "            df_filtered = df\n",
    "        else:\n",
    "            df_filtered = df[df[\"medium_name\"] == medium]\n",
    "    else:\n",
    "        if medium == False:\n",
    "            df_filtered = df[df[\"entity_name\"] == entity]\n",
    "        else:\n",
    "            df_filtered = df[(df[\"medium_name\"] == medium) & (df[\"entity_name\"] == entity)]\n",
    "    \n",
    "    #Setting indeces\n",
    "    indeces = pd.period_range(min(df_filtered[\"pubday\"]), max(df_filtered[\"pubday\"]))\n",
    "    indeces = indeces.to_timestamp()\n",
    "    \n",
    "    #Taking daily average, standard deviation, and sample size\n",
    "    avg = df_filtered.groupby(\"pubday\").mean()\n",
    "    avg.columns = [x + \"_avg\" for x in avg.columns]\n",
    "    std = df_filtered.groupby(\"pubday\").std()\n",
    "    std.columns = [x + \"_std\" for x in std.columns]\n",
    "    sample_size = df_filtered.groupby(\"pubday\").size()\n",
    "    sample_size.name = \"sample_size\"\n",
    "    \n",
    "    #Creating timeseries with interpolation\n",
    "    timeseries = pd.DataFrame(index = indeces)\n",
    "    timeseries = timeseries.join(avg).join(std)\n",
    "    timeseries = timeseries.interpolate(method = interpolation_method)\n",
    "    timeseries = timeseries.dropna()\n",
    "    \n",
    "    #Creating smoothed dataframe\n",
    "    df_smoothed = pd.DataFrame(index = timeseries.index)\n",
    "    \n",
    "    #Fitting simple exponential smoothing and fetching values\n",
    "    for col in list(avg.columns) + list(std.columns):\n",
    "        model = exp_smoothing_method(timeseries[col]).fit(smoothing_level = smoothing_level)\n",
    "        smoothed_values = model.fittedvalues\n",
    "        df_smoothed[col] = smoothed_values\n",
    "    \n",
    "    #Adding sample size\n",
    "    df_smoothed = df_smoothed.join(sample_size).fillna(0)\n",
    "    \n",
    "    #Dropping ID\n",
    "    df_smoothed.drop([\"id_avg\", \"id_std\"], axis = 1, inplace = True)\n",
    "    \n",
    "    #Returning information\n",
    "    if sentiment_col == False:\n",
    "        return df_smoothed\n",
    "    else:\n",
    "        return df_smoothed[sentiment_col+\"_avg\"], df_smoothed[sentiment_col+\"_std\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2: <a class=\"anchor\" id=\"section_3_2\"></a> TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving timeseries outputs for all smoothing methods\n",
    "moving_avg, _ = calculate_moving_avg_std(df_textblob, False, False, 50, \"clause_sentiment\")\n",
    "sg_avg, _ = calculate_savitzky_golay_smoothed_avg_std(df_textblob, False, False, \"time\", 201, 2, \"clause_sentiment\")\n",
    "box_avg, _ = calculate_kernel_smoothed_avg_std(df_textblob, False, False, \"time\", Box1DKernel, 50, \"clause_sentiment\")\n",
    "gaussian_avg, _ = calculate_kernel_smoothed_avg_std(df_textblob, False, False, \"time\", Gaussian1DKernel, 30, \"clause_sentiment\")\n",
    "trapezoid_avg, _ = calculate_kernel_smoothed_avg_std(df_textblob, False, False, \"time\", Trapezoid1DKernel, 60, \"clause_sentiment\")\n",
    "triangle_avg, _ = calculate_triangle_smoothed_avg_std(df_textblob, False, False, \"time\", 50, \"clause_sentiment\")\n",
    "lowess_avg, _ = calculate_lowess_smoothed_avg_std(df_textblob, False, False, \"time\", 0.1, \"clause_sentiment\")\n",
    "simple_exp_avg, _ = calculate_exp_smoothed_avg_std(df_textblob, False, False, SimpleExpSmoothing, \"time\", 0.025, \"clause_sentiment\")\n",
    "holt_avg, _ = calculate_exp_smoothed_avg_std(df_textblob, False, False, Holt, \"time\", 0.025, \"clause_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating timeseries dataframe\n",
    "timeseries = [moving_avg, sg_avg, box_avg, gaussian_avg, trapezoid_avg, triangle_avg, lowess_avg, simple_exp_avg, holt_avg]\n",
    "timeseries_labels = [\"moving\", \"savitzky_golay\", \"box\", \"gaussian\", \"trapezoid\", \"triangle\", \"lowess\", \"simple_exp\", \"holt\"]\n",
    "df_textblob_timeseries = pd.DataFrame(data = {key: value for key, value in zip(timeseries_labels, timeseries)}, \n",
    "                                      index = sg_avg.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to CSV\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df_textblob_timeseries.to_csv(\"Outputs/Articles/Lexicon/textblob_timeseries.csv\")\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.3: <a class=\"anchor\" id=\"section_3_3\"></a> SentiWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving timeseries outputs for all smoothing methods\n",
    "moving_avg, _ = calculate_moving_avg_std(df_sentiws, False, False, 50, \"clause_sentiment\")\n",
    "sg_avg, _ = calculate_savitzky_golay_smoothed_avg_std(df_sentiws, False, False, \"time\", 201, 2, \"clause_sentiment\")\n",
    "box_avg, _ = calculate_kernel_smoothed_avg_std(df_sentiws, False, False, \"time\", Box1DKernel, 50, \"clause_sentiment\")\n",
    "gaussian_avg, _ = calculate_kernel_smoothed_avg_std(df_sentiws, False, False, \"time\", Gaussian1DKernel, 30, \"clause_sentiment\")\n",
    "trapezoid_avg, _ = calculate_kernel_smoothed_avg_std(df_sentiws, False, False, \"time\", Trapezoid1DKernel, 60, \"clause_sentiment\")\n",
    "triangle_avg, _ = calculate_triangle_smoothed_avg_std(df_sentiws, False, False, \"time\", 50, \"clause_sentiment\")\n",
    "lowess_avg, _ = calculate_lowess_smoothed_avg_std(df_sentiws, False, False, \"time\", 0.1, \"clause_sentiment\")\n",
    "imple_exp_avg, _ = calculate_exp_smoothed_avg_std(df_sentiws, False, False, SimpleExpSmoothing, \"time\", 0.025, \"clause_sentiment\")\n",
    "holt_avg, _ = calculate_exp_smoothed_avg_std(df_sentiws, False, False, Holt, \"time\", 0.025, \"clause_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating timeseries dataframe\n",
    "timeseries = [moving_avg, sg_avg, box_avg, gaussian_avg, trapezoid_avg, triangle_avg, lowess_avg, simple_exp_avg, holt_avg]\n",
    "timeseries_labels = [\"moving\", \"savitzky_golay\", \"box\", \"gaussian\", \"trapezoid\", \"triangle\", \"lowess\", \"simple_exp\", \"holt\"]\n",
    "df_sentiws_timeseries = pd.DataFrame(data = {key: value for key, value in zip(timeseries_labels, timeseries)}, \n",
    "                                     index = sg_avg.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to CSV\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df_sentiws_timeseries.to_csv(\"Outputs/Articles/Lexicon/sentiws_timeseries.csv\")\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
