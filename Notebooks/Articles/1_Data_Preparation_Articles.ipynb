{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Chapter 1](#chapter1): Data Cleaning\n",
    "    * [Section 1.1](#section_1_1): Reading in Data\n",
    "    * [Section 1.2](#section_1_2): Handling Datatypes\n",
    "    * [Section 1.3](#section_1_3): Handling Missing Values\n",
    "    * [Section 1.4](#section_1_4): Handling Impossible Data\n",
    "    * [Section 1.5](#section_1_5): Feature Engineering\n",
    "* [Chapter 2](#chapter2): Transforming Data Structure for Sentiment Analysis\n",
    "    * [Section 2.1](#section_2_1): Entity Recognition \n",
    "    * [Section 2.2](#section_2_2): Extracting Passages\n",
    "    * [Section 2.3](#section_2_3): Preparing Passages for Aspect-Based Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: <a class=\"anchor\" id=\"chapter1\"></a> Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing required installations and downloads\n",
    "#pip install spacy\n",
    "#pip install pyvis\n",
    "#python -m spacy.de.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "#TSV file processing\n",
    "import lzma\n",
    "\n",
    "#Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools as it\n",
    "import typing as tp\n",
    "import string as st\n",
    "\n",
    "#NLP\n",
    "from __future__ import print_function, unicode_literals\n",
    "import spacy\n",
    "import networkx as nx\n",
    "from spacy.language import Language\n",
    "\n",
    "#Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.patches as mpatches\n",
    "import pyvis\n",
    "from pyvis.network import Network\n",
    "\n",
    "#Stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "#Other\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suppressing warnings\n",
    "warnings.simplefilter(action = \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in CSVs\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df_uncleaned = pd.read_csv(\"Data/Articles/2022-04-11_swissdox_goebel.csv\", index_col = 0)\n",
    "\n",
    "df_entities = pd.read_csv(\"Inputs/Articles/entities.csv\")\n",
    "df_key_media = pd.read_csv(\"Inputs/Articles/key_media.csv\", index_col = 0)\n",
    "df_media_map = pd.read_csv(\"Inputs/Articles/media_map.csv\", index_col = 0)\n",
    "df_doctype_map = pd.read_csv(\"Inputs/Articles/doctype_map.csv\", index_col = 0)\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting maps for higher-level aggregation\n",
    "entity_map = {key.encode(\"utf-8\").decode(\"unicode-escape\"): value for key, value in zip(df_entities[\"keyword\"], df_entities[\"designed_entity\"])}\n",
    "media_map = {key: value for key, value in zip(df_media_map[\"medium_name\"].values, df_media_map[\"mapped_medium_name\"].values)}\n",
    "doctype_map = {key: value for key, value in zip(df_doctype_map[\"doctype\"].values, df_doctype_map[\"mapped_doctype\"].values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cash',\n",
       " 'Blick',\n",
       " 'NZZ',\n",
       " 'Aargauer_Zeitung',\n",
       " '20_Minuten',\n",
       " 'Basler_Zeitung',\n",
       " 'SRF',\n",
       " 'Tages_Anzeiger',\n",
       " 'St._Galler_Tagblatt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting key media\n",
    "key_media = list(df_key_media[\"key_media\"])\n",
    "key_media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1: <a class=\"anchor\" id=\"section_1_1\"></a> Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to read TSV file and save as CSV\n",
    "def tsv_to_csv(filepath):\n",
    "    #Reading TSV\n",
    "    file = lzma.open(filepath, mode = 'rt', encoding = 'utf-8')\n",
    "    rows = []\n",
    "    for line in file:\n",
    "        if not line.strip() or line.startswith('#'):\n",
    "            continue\n",
    "        row = line.rstrip().split('\\t')\n",
    "        rows.append(row)\n",
    "    \n",
    "    #Instantiating dataframe\n",
    "    df_uncleaned = pd.DataFrame(data = [],\n",
    "                                columns = read_compressed_tsv('2022-04-11_swissdox_goebel.tsv.xz')[0])\n",
    "    \n",
    "    #Reading TSV file\n",
    "    rows = read_compressed_tsv('2022-04-11_swissdox_goebel.tsv.xz')\n",
    "    rows = rows[1:]\n",
    "\n",
    "    #Filling rows of dataframe\n",
    "    for index, row in enumerate(rows):\n",
    "        df_uncleaned.loc[index] = row   \n",
    "    \n",
    "    #Saving to CSV\n",
    "    os.chdir(\"..\")\n",
    "    os.chdir(\"..\")\n",
    "    df_uncleaned.to_csv(\"2022-04-11_swissdox_goebel.csv\")\n",
    "    os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading TSV file \n",
    "#tsv_to_csv(\"2022-04-11_swissdox_goebel.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2: <a class=\"anchor\" id=\"section_1_2\"></a> Handling Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe for cleaning\n",
    "df_cleaned = df_uncleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 86990 entries, 0 to 86989\n",
      "Data columns (total 15 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   id                   86990 non-null  int64 \n",
      " 1   pubtime              86990 non-null  object\n",
      " 2   medium_code          86990 non-null  object\n",
      " 3   medium_name          86990 non-null  object\n",
      " 4   rubric               55072 non-null  object\n",
      " 5   regional             4672 non-null   object\n",
      " 6   doctype              86990 non-null  object\n",
      " 7   doctype_description  86990 non-null  object\n",
      " 8   language             86990 non-null  object\n",
      " 9   char_count           86990 non-null  int64 \n",
      " 10  dateline             21944 non-null  object\n",
      " 11  head                 86990 non-null  object\n",
      " 12  subhead              4581 non-null   object\n",
      " 13  content_id           86990 non-null  object\n",
      " 14  content              86990 non-null  object\n",
      "dtypes: int64(2), object(13)\n",
      "memory usage: 10.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#Checking datatypes\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing datatypes\n",
    "df_cleaned[\"pubtime\"] = pd.to_datetime(df_cleaned[\"pubtime\"], infer_datetime_format = True, utc = True)\n",
    "df_cleaned[\"id\"] = df_cleaned[\"id\"].astype(\"float\").astype(\"int64\") \n",
    "df_cleaned[\"char_count\"] = df_cleaned[\"char_count\"].astype(\"float\").astype(\"int64\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3: <a class=\"anchor\" id=\"section_1_3\"></a> Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing \"\" with np.nan\n",
    "df_cleaned = df_cleaned.replace(\"\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>share_of_missing_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subhead</th>\n",
       "      <td>94.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regional</th>\n",
       "      <td>94.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dateline</th>\n",
       "      <td>74.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rubric</th>\n",
       "      <td>36.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pubtime</th>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium_code</th>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium_name</th>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctype</th>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctype_description</th>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_count</th>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>head</th>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_id</th>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    share_of_missing_values\n",
       "subhead                               94.7%\n",
       "regional                              94.6%\n",
       "dateline                              74.8%\n",
       "rubric                                36.7%\n",
       "id                                     0.0%\n",
       "pubtime                                0.0%\n",
       "medium_code                            0.0%\n",
       "medium_name                            0.0%\n",
       "doctype                                0.0%\n",
       "doctype_description                    0.0%\n",
       "language                               0.0%\n",
       "char_count                             0.0%\n",
       "head                                   0.0%\n",
       "content_id                             0.0%\n",
       "content                                0.0%"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspecting missing values\n",
    "share_missing_vals_num = (df_cleaned.isnull().sum() / len(df_cleaned))*100\n",
    "share_missing_vals_str = round(share_missing_vals_num,1).astype(str)+\"%\"\n",
    "share_missing_vals_df = pd.DataFrame({\"share_of_missing_values\": share_missing_vals_str, \"share_of_missing_values_num\": share_missing_vals_num})\n",
    "share_missing_vals_df = share_missing_vals_df.sort_values(by = \"share_of_missing_values_num\", ascending = False)\n",
    "share_missing_vals_df = share_missing_vals_df.drop(\"share_of_missing_values_num\", axis = 1)\n",
    "share_missing_vals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns with too many missing values\n",
    "df_cleaned.drop([\"subhead\", \"regional\", \"dateline\", \"rubric\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.4: <a class=\"anchor\" id=\"section_1_4\"></a> Handling Impossible Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping English articles\n",
    "df_cleaned = df_cleaned[df_cleaned[\"language\"] == \"de\"]\n",
    "\n",
    "#Dropping language column\n",
    "df_cleaned.drop(\"language\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping duplicate articles\n",
    "df_cleaned.drop_duplicates(subset = \"content_id\", keep = \"first\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.5: <a class=\"anchor\" id=\"section_1_5\"></a> Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to clean content\n",
    "def clean_text(col):\n",
    "    #HTML markers\n",
    "    #Replacing author and image source markers and all text inbetween with a \" \"\n",
    "    markers = [\"au\", \"ur\"]\n",
    "    for marker in markers:\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(f\"<{marker}.+?>\", \" \", x))\n",
    "        \n",
    "    #Replacing link, text, paragraph, and other markers with a \" \"\n",
    "    markers = [\"a\", \"tx\", \"p\", \"lg\", \"ka\", \"nt\"]\n",
    "    for marker in markers:\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(f\"</{marker}>\", \" \", x))\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(f\"<{marker}>\", \" \", x))\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(f\"<{marker}.+?>\", \" \", x))\n",
    "        \n",
    "    #Replacing \\xad and \\xa0 with a \" \"\n",
    "    markers = [\"\\xad\", \"\\xa0\"]\n",
    "    for marker in markers:\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(marker, \" \", x))\n",
    "\n",
    "    #Replacing table and other markers with a \".\"\n",
    "    markers = [\"td\", \"tr\", \"table\", \"th\", \"br\", \"zt\", \"pre\"]\n",
    "    for marker in markers:\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(f\"</{marker}>\", \".\", x))\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(f\"<{marker}>\", \".\", x))\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(f\"<{marker}.+?>\", \".\", x))\n",
    "\n",
    "    #Replacing \"+++\" with a \".\"\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(\"\\+\\+\\+\", \".\", x))\n",
    "\n",
    "    #Replacing subtitle markers with [SUB]\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(\"</ld>\", \"[SUB]\", x))\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(\"<ld>\", \"[SUB]\", x))\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(\"<ld.+?>\", \"[SUB]\", x))\n",
    "    \n",
    "    #Numbers\n",
    "    #Removing numbers with dots\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(\"[0-9]\\d{0,}\\.\", \"\", x))\n",
    "    \n",
    "    #Removing numbers with commas or thousand markers\n",
    "    symbols = [\",\", \"’\", \"'\"]\n",
    "    for symbol in symbols:\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(\"[0-9]\\d{0,}symbol[0-9]\\d{0,}\", \"\", x))\n",
    "    \n",
    "    #Removing remaining numbers\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(\"[0-9]\\d{0,}\", \"\", x))\n",
    "    \n",
    "    #Symbols\n",
    "    #Removing special symbols\n",
    "    symbols = [\"#\", \"\\\\\", \"<\", \">\", \"@\", \"{\", \"}\", \"~\", \"\\+\", \"\\*\", \"_\"]             \n",
    "    matches_regex = \"|\".join(symbols)\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(matches_regex, \"\", x))\n",
    "    \n",
    "    symbols = [\"&\", \"%\", \"/\", \"(\", \")\"]\n",
    "    for symbol in symbols:\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: x.replace(symbol, \"\"))\n",
    "        \n",
    "    #Replacing \"-\" and \";\" with a \" \":\n",
    "    symbols = [\"-\", \"–\" , \";\"] \n",
    "    for symbol in symbols:\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(symbol, \" \", x))\n",
    "        \n",
    "    #Replacing \":\", \"?\" and \"!\" with a \".\":\n",
    "    symbols = [\":\", \"\\?\", \"!\"] \n",
    "    for symbol in symbols:\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(symbol, \".\", x))\n",
    "    \n",
    "    #Replacing '\"\"', '«', '»', \"'\" and \"’\" with a \" \"\n",
    "    symbols = ['\"\"', '«', '»', \"'\", \"’\", \"‹\", \"›\"]\n",
    "    for symbol in symbols:\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(symbol, \" \", x))\n",
    "    \n",
    "    #Beginning and end of content\n",
    "    #Removing trailing spaces\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: x.strip())\n",
    "\n",
    "    #Sentence stops and subtitle markers\n",
    "    #Removing \" \" before and after \".\"\n",
    "    patterns = [\" +\\.\", \"\\. +\"]\n",
    "    for pattern in patterns:\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(pattern, \".\", x))\n",
    "\n",
    "    #Removing \".\" and \" \" before and after subtitle markers\n",
    "    patterns = [\"\\]\\.+\", \"\\.+\\[\", \"\\] +\", \" +\\[\"]\n",
    "    substitutes = [\"]\", \"[\", \"]\", \"[\"]\n",
    "    for pattern, substitute in zip(patterns, substitutes):\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(pattern, substitute, x))\n",
    "    \n",
    "    #Double spaces and dots\n",
    "    #Removing double spaces\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "    \n",
    "    #Removing double dots\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(\"(\\.){2,}\", \".\", x))\n",
    "    \n",
    "    #Removing comma with space\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(\" +, +\", \", \", x))\n",
    "    \n",
    "    #Spelling\n",
    "    #Replacing Umlaute\n",
    "    umlaute = [\"ä\", \"ö\", \"ü\", \"é\"]\n",
    "    substitutes = [\"ae\", \"oe\", \"ue\", \"e\"]\n",
    "    for umlaut, substitute in zip(umlaute, substitutes):\n",
    "        df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(umlaut, substitute, x))\n",
    "\n",
    "    #Ensuring consistent spelling of \"taskforce\"\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(\"Task-Force\", \"Taskforce\", x))\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: re.sub(\"Task Force\", \"Taskforce\", x))\n",
    "    \n",
    "    #Removing trailing spaces\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to find nth (first or second) substring in string\n",
    "def find_first_or_second_substring_index(string, substring, n):\n",
    "    subtractor = n-1\n",
    "    return string.find(substring, string.find(substring) + subtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping medium name to aggregated medium name\n",
    "df_cleaned[\"medium_name_agg\"] = df_cleaned[\"medium_name\"].apply(lambda x: media_map[x] if x in media_map else x)\n",
    "\n",
    "#Replace spaces\n",
    "df_cleaned[\"medium_name_agg\"] = df_cleaned[\"medium_name_agg\"].apply(lambda x: x.replace(\" \", \"_\"))\n",
    "\n",
    "#Replace double spaces\n",
    "df_cleaned[\"medium_name_agg\"] = df_cleaned[\"medium_name_agg\"].apply(lambda x: x.replace(\"__\", \"_\"))\n",
    "\n",
    "#Replace common patterns\n",
    "patterns = [\".ch\", \"Newsnet_/_\", \"_/_MLZ\"]\n",
    "for pattern in patterns:\n",
    "    df_cleaned[\"medium_name_agg\"] = df_cleaned[\"medium_name_agg\"].apply(lambda x: x.replace(pattern, \"\"))\n",
    "\n",
    "#Replace Umlaute\n",
    "patterns = [\"ä\", \"ö\", \"ü\"]\n",
    "substitutes = [\"ae\", \"oe\", \"ue\"]\n",
    "for pattern, substitute in zip(patterns, substitutes):\n",
    "    df_cleaned[\"medium_name_agg\"] = df_cleaned[\"medium_name_agg\"].apply(lambda x: x.replace(pattern, substitute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping doctype to aggregated doctype\n",
    "df_cleaned[\"doctype_description_agg\"] = df_cleaned[\"doctype_description\"].apply(lambda x: doctype_map[x] if x in doctype_map else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting medium channel\n",
    "df_cleaned[\"channel_description\"] = df_cleaned[\"doctype_description\"].apply(lambda x: \"Online medium\" if x == \"Online medium\" else \"Offline medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning text \n",
    "df_cleaned[\"content_cleaned\"] = df_cleaned[\"content\"]\n",
    "df_cleaned[\"head_cleaned\"] = df_cleaned[\"head\"]\n",
    "\n",
    "clean_text(\"content_cleaned\")\n",
    "clean_text(\"head_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting subtitle\n",
    "df_cleaned[\"subhead_cleaned\"] = df_cleaned[\"content_cleaned\"]\n",
    "df_cleaned[\"subhead_cleaned\"] = df_cleaned[\"subhead_cleaned\"].apply(lambda x: x[find_first_or_second_substring_index(x, \"[SUB]\", 1)  \\\n",
    "                                                            + 5 : find_first_or_second_substring_index(x, \"[SUB]\", 2)] \\\n",
    "                                                            if \"[SUB]\" in x else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing subtitle, all remaining subtitle markers, and trailing spaces from content\n",
    "df_cleaned[\"content_cleaned\"] = df_cleaned[\"content_cleaned\"].apply(lambda x: x[find_first_or_second_substring_index(x, \"[SUB]\", 2)  \\\n",
    "                                                            + 5 : ]\\\n",
    "                                                            if \"[SUB]\" in x else x)\n",
    "df_cleaned[\"content_cleaned\"] = df_cleaned[\"content_cleaned\"].apply(lambda x: x.replace(\"[SUB]\",\"\"))\n",
    "df_cleaned[\"content_cleaned\"] =  df_cleaned[\"content_cleaned\"].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating content_full including title, subtitle, and content\n",
    "df_cleaned[\"content_full\"] = df_cleaned.apply(lambda x: str(x[\"head_cleaned\"]) + \".\" + str(x[\"subhead_cleaned\"]) + \".\" + str(x[\"content_cleaned\"]), axis = 1)\n",
    "df_cleaned[\"content_full_lower\"] = df_cleaned[\"content_full\"].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting character count \n",
    "df_cleaned[\"char_count_cleaned\"] = df_cleaned[\"content_full\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting publication day\n",
    "df_cleaned[\"pubday\"] = df_cleaned[\"pubtime\"].dt.strftime('%Y-%m-%d')\n",
    "df_cleaned[\"pubday\"] = pd.to_datetime(df_cleaned[\"pubday\"], infer_datetime_format = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting publication month\n",
    "df_cleaned[\"pubmonth\"] = df_cleaned[\"pubtime\"].dt.strftime('%Y-%m')\n",
    "df_cleaned[\"pubmonth\"] = pd.to_datetime(df_cleaned[\"pubmonth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping irrelevant columns\n",
    "df_cleaned.drop([\"medium_code\", \n",
    "                 \"medium_name\", \n",
    "                 \"doctype\", \n",
    "                 \"doctype_description\",\n",
    "                 \"char_count\", \n",
    "                 \"head\", \n",
    "                 \"content_id\", \n",
    "                 \"content\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming columns\n",
    "df_cleaned.rename(columns = {\"medium_name_agg\": \"medium_name\", \n",
    "                             \"doctype_description_agg\": \"doctype_description\", \n",
    "                             \"content_cleaned\": \"content\", \n",
    "                             \"head_cleaned\": \"head\", \n",
    "                             \"subhead_cleaned\": \"subhead\", \n",
    "                             \"char_count_cleaned\": \"char_count\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to CSV\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df_cleaned.to_csv(\"Data/Articles/cleaned_data.csv\")\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: <a class=\"anchor\" id=\"chapter2\"></a> Transforming Data Structure for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1: <a class=\"anchor\" id=\"section_2_1\"></a> Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in data\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df_cleaned = pd.read_csv(\"Data/Articles/cleaned_data.csv\", index_col = 0, parse_dates = [\"pubtime\", \"pubday\", \"pubmonth\"])\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining entities with proper names where capitalization is important and non-proper names where capitalization is unimportant\n",
    "entities_proper = [x.encode(\"utf-8\").decode(\"unicode-escape\") for x in df_entities[df_entities[\"proper_noun\"] == 1][\"keyword\"].values]\n",
    "entities_not_proper = [x.encode(\"utf-8\").decode(\"unicode-escape\") for x in df_entities[df_entities[\"proper_noun\"] == 0][\"keyword\"].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: <a class=\"anchor\" id=\"section_2_2\"></a> Extracting Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to filter dataframe by entity mentions\n",
    "def filter_by_entity_mentions(df, entities_list, content_col, entity_col):\n",
    "    matches_regex = \"|\".join(entities_list)\n",
    "    df_filtered = df[df[content_col].str.contains(matches_regex, regex = True)]\n",
    "    df.loc[df_filtered.index, entity_col] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to generate passage\n",
    "def create_passage(row):\n",
    "    if row[\"start_of_article\"] == 1:\n",
    "        passage = row[\"original_sentence\"] + \". \" + row[\"next_sentence\"]\n",
    "    elif row[\"end_of_article\"] == 1:\n",
    "        passage = row[\"previous_sentence\"] + \". \" + row[\"original_sentence\"]\n",
    "    else:\n",
    "        passage = row[\"previous_sentence\"] + \". \" + row[\"original_sentence\"] + \". \" + row[\"next_sentence\"]\n",
    "    return passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to get keyword(s) from string and map them to entity\n",
    "def get_entities(string, entities_list):\n",
    "    keyword_list = [entity for entity in entities_list if bool(re.search(entity, string))]\n",
    "    keyword_list = [entity_map[keyword] for keyword in keyword_list]\n",
    "    keyword_string = str(keyword_list).replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "    return keyword_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to get keyword(s) from string\n",
    "def get_keywords(string, entities_list):\n",
    "    keyword_list = [entity for entity in entities_list if bool(re.search(entity, string))]\n",
    "    keyword_string = str(keyword_list).replace(\"['\",\"\").replace(\"']\",\"\").replace(\"'\",\"\").replace(\"[]\",\"\")\n",
    "    return keyword_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering dataframe by mentions of entity and mark relevant rows (proper names where capitalization is important)\n",
    "df_cleaned[\"entities_proper\"] = 0\n",
    "filter_by_entity_mentions(df_cleaned, entities_proper, \"content_full\", \"entities_proper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering dataframe by mentions of entity and mark relevant rows (not proper names where capitalization is unimportant)\n",
    "df_cleaned[\"entities_not_proper\"] = 0\n",
    "filter_by_entity_mentions(df_cleaned, entities_not_proper, \"content_full_lower\", \"entities_not_proper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating joint column of mentions\n",
    "df_cleaned[\"entities\"] = df_cleaned.apply(lambda x: 1 if (x[\"entities_proper\"] == 1) | (x[\"entities_not_proper\"] == 1) else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new dataframe, containing only articles which mention at least one entity\n",
    "df = df_cleaned[df_cleaned[\"entities\"] == 1].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataframe by sentence\n",
    "df[\"content_full\"] = df[\"content_full\"].apply(lambda x: x.split(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploding dataframe to sentence-level\n",
    "df = df.explode(\"content_full\").reset_index(drop = True)\n",
    "df[\"content_full\"] = df[\"content_full\"].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming columns\n",
    "df.rename(columns = {\"content\": \"original_content\",\n",
    "                     \"content_full\": \"original_sentence\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping irrelevant columns\n",
    "df.drop([\"head\", \"subhead\", \"entities_proper\", \"entities_not_proper\", \"entities\", \"content_full_lower\"], \n",
    "       axis = 1, \n",
    "       inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows with empty content\n",
    "df = df[df[\"original_sentence\"] != \"\"].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recording previous and following sentence\n",
    "df[\"previous_sentence\"] = df[\"original_sentence\"].shift(1, fill_value = \"\")\n",
    "df[\"next_sentence\"] = df[\"original_sentence\"].shift(-1, fill_value = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recording whether sentence marks start or end of an article\n",
    "df[\"start_of_article\"] = np.where(df[\"id\"] != df[\"id\"].shift(1, fill_value = 0), 1, 0)\n",
    "df[\"end_of_article\"] = np.where(df[\"id\"] != df[\"id\"].shift(-1, fill_value = 0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating passage\n",
    "df[\"original_passage\"] = df.apply(lambda x: create_passage(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping irrelevant columns\n",
    "df.drop([\"previous_sentence\", \"next_sentence\", \"start_of_article\", \"end_of_article\"], \n",
    "        axis = 1, \n",
    "        inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating lowercase content_full and passage\n",
    "df[\"original_sentence_lower\"] = df[\"original_sentence\"].apply(lambda x: x.lower())\n",
    "df[\"original_passage_lower\"] = df[\"original_passage\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering dataframe by mentions of entity and mark relevant rows (proper names where capitalization is important)\n",
    "df[\"entities_proper\"] = 0\n",
    "filter_by_entity_mentions(df, entities_proper, \"original_sentence\", \"entities_proper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering dataframe by mentions of entity and mark relevant rows (not proper names where capitalization is unimportant)\n",
    "df[\"entities_not_proper\"] = 0\n",
    "filter_by_entity_mentions(df, entities_not_proper, \"original_sentence_lower\", \"entities_not_proper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating joint column of mentions\n",
    "df[\"entities\"] = df.apply(lambda x: 1 if (x[\"entities_proper\"] == 1) | (x[\"entities_not_proper\"] == 1) else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping sentences without mentions of at least one entity\n",
    "df = df[df[\"entities\"] == 1].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding columns with entity contained in sentence\n",
    "df[\"entities_proper_names\"] = df[\"original_sentence\"].apply(lambda x: get_entities(x, entities_proper))\n",
    "df[\"entities_not_proper_names\"] = df[\"original_sentence_lower\"].apply(lambda x: get_entities(x, entities_not_proper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating joint column of entities\n",
    "df[\"entities_names\"] = df.apply(lambda x: set(x[\"entities_proper_names\"].split(\", \")).union(set(x[\"entities_not_proper_names\"].split(\", \"))), axis = 1)\n",
    "df[\"entities_names\"] = df[\"entities_names\"].apply(lambda x: str(x).replace(\", ''\",\"\").replace(\"{\",\"\").replace(\"}\",\"\").replace(\"'\",\"\").lstrip(\", \").rstrip(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding columns with entity keywords contained in sentence\n",
    "df[\"entities_proper_keywords\"] = df[\"original_sentence\"].apply(lambda x: get_keywords(x, entities_proper))\n",
    "df[\"entities_not_proper_keywords\"] = df[\"original_sentence_lower\"].apply(lambda x: get_keywords(x, entities_not_proper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating joint column of entity keywords\n",
    "df[\"entities_keywords\"] = df.apply(lambda x: set(x[\"entities_proper_keywords\"].split(\", \")).union(set(x[\"entities_not_proper_keywords\"].split(\", \"))), axis = 1)\n",
    "df[\"entities_keywords\"] = df[\"entities_keywords\"].apply(lambda x: str(x).encode(\"utf-8\").decode(\"unicode-escape\").replace(\", ''\",\"\").replace(\"'\",\"\").lstrip(\"{\").rstrip(\"}\").lstrip(\", \").rstrip(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating column with number of entities\n",
    "df[\"num_entities\"] = df[\"entities_names\"].apply(lambda x: len(x.split(\", \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping irrelevant columns\n",
    "df.drop([\"entities_proper\", \"entities_not_proper\", \"entities\"], \n",
    "        axis = 1, \n",
    "        inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to CSV\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df.to_csv(\"Data/Articles/cleaned_partially_parsed_data.csv\")\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3: <a class=\"anchor\" id=\"section_2_3\"></a> Preparing Passages for Aspect-Based Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in data\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df = pd.read_csv(\"Data/Articles/cleaned_partially_parsed_data.csv\", index_col = 0, parse_dates = [\"pubtime\", \"pubday\", \"pubmonth\"])\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function for dependency parsing\n",
    "def get_dep(string):\n",
    "    tokens = nlp(string)\n",
    "    dep = [token.dep_ for token in tokens]\n",
    "    return dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function for PoS tagging\n",
    "def get_pos(string):\n",
    "    tokens = nlp(string)\n",
    "    pos = [token.pos_ for token in tokens]\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining classes to detect clauses\n",
    "verb_pos = {\"VERB\", \"AUX\"}\n",
    "finite_verb_tags = {\"VVFIN\", \"VMFIN\", \"VAFIN\"}\n",
    "\n",
    "class Clause:\n",
    "    def __init__(self, spans: tp.Iterable[\"spacy.tokens.Span\"]):\n",
    "        self.spans = spans\n",
    "\n",
    "    @property\n",
    "    def __chain(self) -> tp.Iterable[\"spacy.tokens.Token\"]:\n",
    "        return [token for token in it.chain(*self.spans)]\n",
    "\n",
    "    def __getitem__(self, index: int) -> \"spacy.tokens.Token\":\n",
    "        return self.__chain[index]\n",
    "\n",
    "    def __iter__(self) -> tp.Iterator:\n",
    "        self.n = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> \"spacy.tokens.Token\":\n",
    "        self.n += 1\n",
    "        try:\n",
    "            return self[self.n - 1]\n",
    "        except IndexError:\n",
    "            raise StopIteration\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \" \".join([span.text for span in self.inner_spans])\n",
    "\n",
    "    @property\n",
    "    def is_subclause(self) -> bool:\n",
    "        \"\"\"Clause is a subclause iff the last token is a finite verb.\"\"\"\n",
    "        return (\n",
    "            self[-2].tag_ in finite_verb_tags\n",
    "            if self[-1].pos_ == \"PUNCT\"\n",
    "            else self[-1].tag_ in finite_verb_tags)\n",
    "\n",
    "    @property\n",
    "    def clause_type(self) -> str:\n",
    "        return \"SUB\" if self.is_subclause else \"MAIN\"\n",
    "\n",
    "    @property\n",
    "    def inner_spans(self) -> tp.List[\"spacy.tokens.Span\"]:\n",
    "        inner_spans = []\n",
    "        for span in self.spans:\n",
    "            inner_spans.append(span)\n",
    "        return inner_spans\n",
    "\n",
    "\n",
    "class ClausedSentence(spacy.tokens.Span):\n",
    "    @property\n",
    "    def __finite_verb_indices(self) -> tp.List[int]:\n",
    "        return [token.i for token in self if token.tag_ in finite_verb_tags]\n",
    "\n",
    "    def progeny(\n",
    "        self,\n",
    "        index: int,\n",
    "        stop_indices: tp.Optional[tp.List[int]] = None) -> tp.List[\"spacy.tokens.Token\"]:\n",
    "        if stop_indices is None:\n",
    "            stop_indices = []\n",
    "\n",
    "        progeny = [index]\n",
    "\n",
    "        for child in self[index].children:\n",
    "            if child.i in stop_indices:\n",
    "                continue\n",
    "\n",
    "            progeny += [child.i] + self.progeny(child.i, stop_indices)\n",
    "\n",
    "        return sorted(list(set(progeny)))\n",
    "\n",
    "    @property\n",
    "    def clauses(self) -> tp.Generator[\"Clause\", None, None]:\n",
    "        for verb_index in self.__finite_verb_indices:\n",
    "            clause_tokens = [\n",
    "                self[index]\n",
    "                for index in self.progeny(\n",
    "                    index=verb_index, stop_indices = self.__finite_verb_indices)]\n",
    "\n",
    "            spans = []\n",
    "\n",
    "            for _, group in it.groupby(\n",
    "                enumerate(clause_tokens),\n",
    "                lambda index_token: index_token[0] - index_token[1].i):\n",
    "                tokens = [item[1] for item in group]\n",
    "                spans.append(self[tokens[0].i : tokens[-1].i + 1])\n",
    "\n",
    "            yield Clause(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to get subclause for given entity\n",
    "def get_subclause(string, entity):\n",
    "    tokens = nlp(string)\n",
    "    try:\n",
    "        sentences = tokens.sents \n",
    "        sentence = next(sentences)\n",
    "        claused_sentence = ClausedSentence(sentence.doc, sentence.start, sentence.end)\n",
    "        clauses = list(claused_sentence.clauses)\n",
    "        relevant_clauses = []\n",
    "        for clause in clauses:\n",
    "            regex_match = re.search(\"\\w*\" + entity + \"\\w*\", str(clause))\n",
    "            if regex_match:\n",
    "                clause = re.sub(\",\", \"\", str(clause))\n",
    "                clause = re.sub(\" +\", \" \", clause)\n",
    "                clause = re.sub(\"\\.\", \"\", clause)\n",
    "\n",
    "                relevant_clauses.append(clause)\n",
    "        return relevant_clauses\n",
    "    except:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to get most related words for a given entity\n",
    "def get_related_words(string, entity):\n",
    "    #Get tokens\n",
    "    tokens = nlp(string)\n",
    "    tokens_text = [str(token) for token in tokens]\n",
    "    \n",
    "    #Get index of entity and surrounding 3 words on each side\n",
    "    for token_text in tokens_text:\n",
    "        regex_match = re.search(\"\\w*\" + entity + \"\\w*\", token_text)\n",
    "        if regex_match:\n",
    "            entity_form = regex_match.group(0)\n",
    "            break\n",
    "    try:\n",
    "        entity_idx = tokens_text.index(entity_form)\n",
    "    except:\n",
    "        return string\n",
    "    idx_range_max = min(entity_idx + 3, len(tokens)-1)\n",
    "    idx_range_min = max(0, entity_idx - 3)\n",
    "    idx_range = [x for x in range(idx_range_min, idx_range_max + 1)]\n",
    "    \n",
    "    #Get document length\n",
    "    doc_len = len(tokens)\n",
    "    \n",
    "    #Add children into network \n",
    "    nodes = []\n",
    "    edges = []\n",
    "    for token in tokens:\n",
    "        nodes.append(str(token))\n",
    "        for child in token.children:\n",
    "            edges.append((str(token),\n",
    "                          str(child)))\n",
    "    \n",
    "    #Create network\n",
    "    graph = nx.Graph(edges)\n",
    "    \n",
    "    #Extract relevant tokens\n",
    "    relevant_tokens = []\n",
    "    for token, token_text in zip(tokens, tokens_text):\n",
    "        #If token in immediate surrounding of entity, add token to relevant tokens\n",
    "        token_idx = tokens_text.index(token_text)\n",
    "        if token_idx in idx_range:\n",
    "            relevant_tokens.append(token_text)\n",
    "        \n",
    "        #Calculate path distance between entity and token\n",
    "        try:\n",
    "            path_length = nx.shortest_path_length(graph, source = entity_form, target = token_text)\n",
    "        except:\n",
    "            path_length = 0\n",
    "        path_length_normalized = path_length / doc_len\n",
    "        #print(token_text + \" \" + str(round(path_length_normalized,2)))\n",
    "        \n",
    "        #If token within short distance of entity in network, add token to relevant tokens\n",
    "        if (path_length_normalized <= 0.2) & (token_text not in relevant_tokens):\n",
    "            relevant_tokens.append(token_text)\n",
    "        elif (path_length_normalized <= 0.3) & ((token.pos_ == \"ADV\")|(token.pos_ == \"ADJ\")) & (token_text not in relevant_tokens):\n",
    "            relevant_tokens.append(token_text)\n",
    "    \n",
    "    return relevant_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to replace entity keywords with entities in sentence\n",
    "def replace_entity_mentions(string, designed_entity):\n",
    "    entity_keywords = [key for key, value in zip(entity_map.keys(), entity_map.values()) if value == designed_entity]\n",
    "    matches_regex = \"|\".join(entity_keywords)\n",
    "    string_new = re.sub(matches_regex, designed_entity, string)\n",
    "    return string_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating nlp\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.custom_boundaries(doc)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining custom sentence boundaries for clause detector\n",
    "#nlp.remove_pipe(\"custom_boundaries\")\n",
    "\n",
    "@Language.component(\"custom_boundaries\")\n",
    "def custom_boundaries(doc):\n",
    "    for token in doc:\n",
    "        doc[token.i].is_sent_start = False\n",
    "    for token in doc:\n",
    "        if token.text == \".\":\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "#Language.factory(\"custom_boundaries\", func = custom_boundaries)\n",
    "\n",
    "nlp.add_pipe(\"custom_boundaries\", before = \"parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataframe on entities\n",
    "df[\"entities_keywords\"] = df[\"entities_keywords\"].apply(lambda x: x.split(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploding dataframe to entity-level\n",
    "df = df.explode(\"entities_keywords\").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming columns \n",
    "df.rename(columns = {\"entities_names\": \"entity_name\", \n",
    "                     \"entities_keywords\": \"entity_keyword\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping keyword to entity\n",
    "df[\"entity_name\"] = df[\"entity_keyword\"].apply(lambda x: entity_map[x.encode(\"utf-8\").decode(\"unicode-escape\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing entity keywords with entities\n",
    "df[\"sentence_ABSA\"] = df.apply(lambda x: replace_entity_mentions(x[\"original_sentence_lower\"], x[\"entity_name\"]) \\\n",
    "                               if x[\"entity_keyword\"].islower() else \\\n",
    "                               replace_entity_mentions(x[\"original_sentence\"], x[\"entity_name\"]),\n",
    "                               axis = 1)\n",
    "df[\"passage_ABSA\"] = df.apply(lambda x: replace_entity_mentions(x[\"original_passage_lower\"], x[\"entity_name\"]) \\\n",
    "                              if x[\"entity_keyword\"].islower() else \\\n",
    "                              replace_entity_mentions(x[\"original_passage\"], x[\"entity_name\"]),\n",
    "                              axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 271577/271577 [21:16<00:00, 212.75it/s]\n"
     ]
    }
   ],
   "source": [
    "#Getting related keywords per entity\n",
    "df[\"sentence_ABSA_rel_keywords\"] = df.progress_apply(lambda x: get_related_words(x[\"sentence_ABSA\"], x[\"entity_name\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning back into string\n",
    "df[\"sentence_ABSA_rel_keywords\"] = df[\"sentence_ABSA_rel_keywords\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 271577/271577 [19:57<00:00, 226.80it/s]\n"
     ]
    }
   ],
   "source": [
    "#Getting subclauses per entity\n",
    "df[\"sentence_ABSA_subclause\"] = df.progress_apply(lambda x: get_subclause(x[\"sentence_ABSA\"], x[\"entity_name\"].lower), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning back into string\n",
    "df[\"sentence_ABSA_subclause\"] = df[\"sentence_ABSA_subclause\"].apply(lambda x: str(x).replace(\"[]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercasing\n",
    "columns = [\"sentence_ABSA\", \"passage_ABSA\", \"sentence_ABSA_rel_keywords\", \"sentence_ABSA_subclause\"]\n",
    "\n",
    "for column in columns:\n",
    "    df[column] = df[column].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing polarized entity keywords from sentence\n",
    "polarized_entities = [\"Befuerworter\", \"Gegner\", \"Skeptiker\", \"Kritiker\", \"Opposition\", \"Demonstranten\"]\n",
    "matches_regex = \"|\".join(polarized_entities)\n",
    "columns = [\"sentence_ABSA\", \"passage_ABSA\", \"sentence_ABSA_rel_keywords\", \"sentence_ABSA_subclause\"]\n",
    "\n",
    "for column in columns:\n",
    "    df[column] = df[column].apply(lambda x: re.sub(matches_regex, \"[NEG_ENT]\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows where BAG infoline number is indicated\n",
    "df = df[~df[\"sentence_ABSA\"].str.contains(\"bag infoline\", regex = False)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to create relevant clause\n",
    "def create_clause(number_entities, sentence, subclause):\n",
    "    \"\"\"Create clause with full sentence, if only one entity is mentioned or subclause is too small, \n",
    "    and subclause otherwise\"\"\"\n",
    "    length_subclause = len(subclause)\n",
    "    if (number_entities > 1) & (length_subclause > 5):\n",
    "        return subclause\n",
    "    else:\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new column with relevant clause\n",
    "df[\"clause_ABSA\"] = df.apply(lambda x: create_clause(x[\"num_entities\"], \n",
    "                                                     x[\"sentence_ABSA\"], \n",
    "                                                     x[\"sentence_ABSA_subclause\"]), \n",
    "                             axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping irrelevant columns\n",
    "df.drop([\"original_sentence_lower\", \"original_passage_lower\",\n",
    "         \"entities_proper_names\", \"entities_not_proper_names\", \n",
    "         \"entities_proper_keywords\", \"entities_not_proper_keywords\"], \n",
    "        axis = 1, \n",
    "        inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to CSV\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df.to_csv(\"Data/Articles/cleaned_parsed_data.csv\")\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
