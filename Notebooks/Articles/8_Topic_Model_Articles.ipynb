{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Chapter 1](#chapter1): Preprocessing\n",
    "* [Chapter 2](#chapter2): Topic Modelling\n",
    "    * [Section 2.1](#section_2_1): LDA\n",
    "    * [Section 2.2](#section_2_2): NMF\n",
    "    * [Section 2.3](#section_2_3): Word Embeddings\n",
    "* [Chapter 3](#chapter3): Visualization Preparation\n",
    "    * [Section 3.1](#section_3_1): Dimensionality Reduction\n",
    "    * [Section 3.2](#section_3_2): Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: <a class=\"anchor\" id=\"chapter1\"></a> Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing required installations\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "#Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "\n",
    "#Natural language processing\n",
    "from nltk.corpus import stopwords\n",
    "import string as st\n",
    "import spacy\n",
    "\n",
    "#Topic modelling\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Network analysis\n",
    "from pyvis.network import Network\n",
    "import itertools\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.patches as mpatches\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim_models as gensimvis\n",
    "#pyLDAvis.enable_notebook()\n",
    "\n",
    "#Other\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suppressing warnings\n",
    "warnings.simplefilter(action = \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in CSVs\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "df = pd.read_csv(\"Outputs/Articles/df_final.csv\", index_col = 0, parse_dates = [\"pubtime\", \"pubday\", \"pubmonth\"])\n",
    "df_preproc = pd.read_csv(\"Outputs/Articles/df_final_preprocessed.csv\", index_col = 0, parse_dates = [\"pubtime\", \"pubday\", \"pubmonth\"])\n",
    "\n",
    "df_entities = pd.read_csv(\"Inputs/Articles/entities.csv\")\n",
    "df_key_entities = pd.read_csv(\"Inputs/Articles/key_entities_large.csv\", index_col = 0)\n",
    "df_key_entities_small = pd.read_csv(\"Inputs/Articles/key_entities.csv\", index_col = 0)\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ueli_Maurer',\n",
       " 'Guy_Parmelin',\n",
       " 'Ignazio_Cassis',\n",
       " 'Karin_Keller_Sutter',\n",
       " 'Simonetta_Sommaruga',\n",
       " 'Alain_Berset',\n",
       " 'Viola_Amherd',\n",
       " 'Bundesrat',\n",
       " 'Tanja_Stadler',\n",
       " 'Marcel_Tanner',\n",
       " 'Martin_Ackermann',\n",
       " 'Matthias_Egger',\n",
       " 'Taskforce',\n",
       " 'Christoph_Berger',\n",
       " 'EKIF',\n",
       " 'Stefan_Kuster',\n",
       " 'Pascal_Strupler',\n",
       " 'Virginie_Masserey',\n",
       " 'Anne_Levy',\n",
       " 'Patrick_Mathys',\n",
       " 'Marcel_Salathe',\n",
       " 'Daniel_Koch',\n",
       " 'BAG',\n",
       " 'Swissmedic',\n",
       " 'Lukas_Engelberger',\n",
       " 'GDK',\n",
       " 'SVP',\n",
       " 'SP',\n",
       " 'FDP',\n",
       " 'Die_Mitte',\n",
       " 'Die_Gruene',\n",
       " 'Gruenliberale',\n",
       " 'Juso',\n",
       " 'Befuerworter',\n",
       " 'Ja_Lager',\n",
       " 'Gegner',\n",
       " 'Leugner',\n",
       " 'Skeptiker',\n",
       " 'Kritiker',\n",
       " 'Opposition',\n",
       " 'Nein_Lager',\n",
       " 'Demonstranten',\n",
       " 'Freunde_Der_Verfassung',\n",
       " 'Mass_Voll']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting entities\n",
    "entities = list(df_entities[\"designed_entity\"].unique())\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ueli_Maurer',\n",
       " 'Alain_Berset',\n",
       " 'Bundesrat',\n",
       " 'Taskforce',\n",
       " 'BAG',\n",
       " 'Daniel_Koch',\n",
       " 'Tanja_Stadler',\n",
       " 'Christoph_Berger',\n",
       " 'Lukas_Engelberger',\n",
       " 'Patrick_Mathys',\n",
       " 'SVP',\n",
       " 'SP',\n",
       " 'FDP',\n",
       " 'Die_Mitte']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting key entities\n",
    "key_entities_small = list(df_key_entities_small[\"key_entities\"])\n",
    "key_entities_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating nlp\n",
    "nlp = spacy.load(\"de_core_news_md\", disable = [\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to lemmatize tokens\n",
    "def lemmatize(tokens):\n",
    "    spacy_tokens = [nlp(token) for token in tokens]\n",
    "    lemmas = [spacy_token[0].lemma_ for spacy_token in spacy_tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to remove entity from tokens\n",
    "def remove_entity(tokens, entity):\n",
    "    names = entity.split(\"_\")\n",
    "    regex_match_list = [\"\\w*\" + name + \"\\w*\" for name in names]\n",
    "    regex_match = \"|\".join(regex_match_list)\n",
    "    tokens = [x for x in tokens if re.search(regex_match, x) == None]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining preprocessing function\n",
    "def preprocess(df):\n",
    "    #Creating new columns\n",
    "    df[\"passage_tokens\"] = df[\"original_passage\"]\n",
    "      \n",
    "    #Tokenizing\n",
    "    df[\"passage_tokens\"] = df[\"passage_tokens\"].progress_apply(lambda x: x.split())\n",
    "    \n",
    "    #Removing stopwords\n",
    "    stopword = set(stopwords.words(\"german\"))\n",
    "    \n",
    "    df[\"passage_tokens\"] = df[\"passage_tokens\"].progress_apply(lambda x: [token for token in x if token not in stopword])\n",
    "        \n",
    "    #Removing punctuation\n",
    "    punctuation = list(st.punctuation)\n",
    "    \n",
    "    df[\"passage_tokens\"] = df[\"passage_tokens\"].progress_apply(lambda x: [token for token in x if token not in punctuation])\n",
    "        \n",
    "    #Removing [NEG_ENT] tokens\n",
    "    df[\"passage_tokens\"] = df[\"passage_tokens\"].progress_apply(lambda x: [token for token in x if token != \"[NEG_ENT]\"])\n",
    "    \n",
    "    #Removing entity names\n",
    "    df[\"passage_tokens\"] = df.progress_apply(lambda x: remove_entity(x[\"passage_tokens\"], x[\"entity_name\"].lower()), axis = 1)\n",
    "\n",
    "    #Lemmatizing\n",
    "    df[\"passage_tokens_lemmatized\"] = df[\"passage_tokens\"].progress_apply(lambda x: lemmatize(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping duplicates\n",
    "#df_unique = df.drop_duplicates(subset = [\"sentence_ABSA\", \"entity_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "#df_preproc = preprocess(df_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to CSV\n",
    "#os.chdir(\"..\")\n",
    "#os.chdir(\"..\")\n",
    "#df_preproc.to_csv(\"Outputs/Articles/df_final_preprocessed.csv\")\n",
    "#os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering to negative statements only\n",
    "df_filtered = df_preproc[df_preproc[\"sentiment\"] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting tokens to list\n",
    "df_filtered[\"passage_tokens_lemmatized\"] = df_filtered[\"passage_tokens_lemmatized\"].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: <a class=\"anchor\" id=\"chapter2\"></a> Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1: <a class=\"anchor\" id=\"section_2_1\"></a> LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to identify topics of criticism for given entity\n",
    "def get_topics_of_criticism_lda(entity, num_topics, no_above_dict, tfidf, alpha, eta, decay):\n",
    "    #Filtering by entity\n",
    "    data = df_filtered[df_filtered[\"entity_name\"] == entity]\n",
    "    \n",
    "    #Creating dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(data[\"passage_tokens_lemmatized\"])\n",
    "    dictionary.filter_extremes(no_below = 10, no_above = no_above_dict)\n",
    "    \n",
    "    #Creating Doc2Bow corpus\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data[\"passage_tokens_lemmatized\"]]\n",
    "    \n",
    "    #TF-IDF vectorizing corpus\n",
    "    tf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tf[corpus]\n",
    "    \n",
    "    #Generating final corpus\n",
    "    if tfidf == False:\n",
    "        corpus_final = corpus\n",
    "    else:\n",
    "        corpus_final = corpus_tfidf\n",
    "        \n",
    "    #Fitting LDA model on count vectorized corpus\n",
    "    lda_model = gensim.models.LdaMulticore(corpus_final, \n",
    "                                           num_topics = num_topics, \n",
    "                                           id2word = dictionary, \n",
    "                                           alpha = alpha,\n",
    "                                           eta = eta,\n",
    "                                           decay = decay,\n",
    "                                           passes = 5,\n",
    "                                           workers = 5)\n",
    "    \n",
    "    #Calculating perplexity\n",
    "    perplexity = lda_model.log_perplexity(corpus_final)\n",
    "        \n",
    "    #Calculating coherence\n",
    "    coherence_model = CoherenceModel(model = lda_model, \n",
    "                                     texts = data[\"passage_tokens_lemmatized\"], \n",
    "                                     dictionary = dictionary, \n",
    "                                     coherence = \"c_v\")\n",
    "    coherence = coherence_model.get_coherence()\n",
    "    \n",
    "    return lda_model, corpus_final, perplexity, coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to save topics\n",
    "def generate_lda_topics(num_topics, no_above_dict, tfidf, alpha, eta, decay, entities):\n",
    "    topics_df = pd.DataFrame(index = [x for x in range(num_topics)])\n",
    "    models = []\n",
    "    corpora = []\n",
    "    perplexities = []\n",
    "    coherences = []\n",
    "    \n",
    "    for entity in entities:\n",
    "        try:\n",
    "            model, corpus, perplexity, coherence = get_topics_of_criticism_lda(entity,\n",
    "                                                                               num_topics,\n",
    "                                                                               no_above_dict,\n",
    "                                                                               tfidf, \n",
    "                                                                               alpha, \n",
    "                                                                               eta, \n",
    "                                                                               decay)\n",
    "            all_words = []\n",
    "            for index in range(num_topics):\n",
    "                words = model.print_topics(-1)[index][1].split(\"+\")\n",
    "                words = [re.sub(\"[0-9]\\d{0,}\\.[0-9]\\d{0,}\", \"\", x) for x in words]\n",
    "                words = [re.sub(\"\\*\", \"\", x) for x in words]\n",
    "                words = [re.sub('\"', \"\", x) for x in words]\n",
    "                words = [re.sub(\"\\ \", \"\", x) for x in words]\n",
    "                all_words.append(words)\n",
    "            topics_df[entity] = all_words\n",
    "\n",
    "            models.append(model)\n",
    "            corpora.append(corpus)\n",
    "            perplexities.append(perplexity)\n",
    "            coherences.append(coherence)\n",
    "        \n",
    "        except:\n",
    "            topics_df[entity] = 0\n",
    "            \n",
    "    avg_perplexity = np.mean(perplexities)\n",
    "    avg_coherence = np.mean(coherences)\n",
    "    \n",
    "    return topics_df, models, corpora, avg_perplexity, avg_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting topics\n",
    "lda_topics_df, lda_models, lda_corpora, perplexity_lda, coherence_lda = generate_lda_topics(3, 0.6, True, \"symmetric\", \"auto\", 0, key_entities_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  -5.0\n",
      "\n",
      "Coherence:  0.3\n"
     ]
    }
   ],
   "source": [
    "#Printing evaluation metrics\n",
    "print(\"Perplexity: \", round(perplexity_lda,1))\n",
    "print(\"\\nCoherence: \", round(coherence_lda,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: <a class=\"anchor\" id=\"section_2_2\"></a> NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to identify topics of criticism for given entity\n",
    "def get_topics_of_criticism_nmf(entity, num_topics, tfidf, alpha, l1_ratio):\n",
    "    #Filtering by entity\n",
    "    data = df_filtered[df_filtered[\"entity_name\"] == entity]\n",
    "    \n",
    "    #Count vectorizing corpus\n",
    "    cv = CountVectorizer()\n",
    "    cv.fit(data[\"passage_tokens_lemmatized\"].apply(lambda x: \" \".join(x)))\n",
    "    passage_cv = cv.transform(data[\"passage_tokens_lemmatized\"].apply(lambda x: \" \".join(x)))\n",
    "    \n",
    "    #TF-IDF vectorizing corpus\n",
    "    tf = TfidfVectorizer()\n",
    "    tf.fit(data[\"passage_tokens_lemmatized\"].apply(lambda x: \" \".join(x)))\n",
    "    passage_tf = tf.transform(data[\"passage_tokens_lemmatized\"].apply(lambda x: \" \".join(x)))\n",
    "    \n",
    "    nmf_model = NMF(n_components = num_topics, \n",
    "                    alpha = alpha, \n",
    "                    l1_ratio = l1_ratio,\n",
    "                    random_state = 1)\n",
    "    if tfidf == False:\n",
    "        #Fitting NMF model on count vectorized corpus\n",
    "        nmf = nmf_model.fit(passage_cv)\n",
    "        components_df = pd.DataFrame(nmf_model.components_, columns = cv.get_feature_names())\n",
    "    else:\n",
    "        #Fitting NMF model on TF-IDF vectorized corpus\n",
    "        nmf = nmf_model.fit(passage_tf)\n",
    "        components_df = pd.DataFrame(nmf_model.components_, columns = tf.get_feature_names())\n",
    "    \n",
    "    return components_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to save topics\n",
    "def generate_nmf_topics(num_topics_dict, tfidf, alpha_dict, l1_dict, entities):\n",
    "    topics_df = pd.DataFrame(index = [x for x in range(max(num_topics_dict.values()))])\n",
    "    components_dfs = []\n",
    "    \n",
    "    for entity in entities:\n",
    "        try:\n",
    "            alpha = alpha_dict[entity]\n",
    "            l1_ratio = l1_dict[entity]\n",
    "            num_topics = num_topics_dict[entity]\n",
    "        except:\n",
    "            alpha = 0.1\n",
    "            l1_ratio = 0.1\n",
    "            num_topics = 3\n",
    "        \n",
    "        try:\n",
    "            components_df = get_topics_of_criticism_nmf(entity, \n",
    "                                                            num_topics,\n",
    "                                                            tfidf, \n",
    "                                                            alpha, \n",
    "                                                            l1_ratio)\n",
    "            for index in range(num_topics):\n",
    "                tmp = components_df.iloc[index]\n",
    "                words = list(tmp.nlargest(15).index)\n",
    "                topics_df.at[index, entity] = \", \".join(words)\n",
    "            components_dfs.append(components_df)\n",
    "        except:\n",
    "            topics_df[entity] = 0\n",
    "            components_dfs.append(0)\n",
    "            \n",
    "    return topics_df, components_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting parameters\n",
    "alpha_dict = {\"Ueli_Maurer\": 0.1, \n",
    "              \"Alain_Berset\": 0.1,\n",
    "              \"Bundesrat\": 0.075, \n",
    "              \"Taskforce\": 0.1, \n",
    "              \"BAG\": 0.2, \n",
    "              \"Daniel_Koch\": 0.2}\n",
    "l1_dict = {\"Ueli_Maurer\": 0.6, \n",
    "           \"Alain_Berset\": 0.6, \n",
    "           \"Bundesrat\": 0.7, \n",
    "           \"Taskforce\": 0.1, \n",
    "           \"BAG\": 0.5, \n",
    "           \"Daniel_Koch\": 0.4}\n",
    "num_topics_dict = {\"Ueli_Maurer\": 5, \n",
    "                   \"Alain_Berset\": 7, \n",
    "                   \"Bundesrat\": 4, \n",
    "                   \"Taskforce\": 5, \n",
    "                   \"BAG\": 3, \n",
    "                   \"Daniel_Koch\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting topics\n",
    "nmf_topics_df, nmf_components_dfs = generate_nmf_topics(num_topics_dict, True, alpha_dict, l1_dict, key_entities_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in CSV\n",
    "#os.chdir(\"..\")\n",
    "#os.chdir(\"..\")\n",
    "#nmf_labelled_topics_df = pd.read_csv(\"Outputs/Articles/Topic Models/nmf.csv\", index_col = 0)\n",
    "#os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming columns\n",
    "nmf_labelled_topics_df = nmf_topics_df.copy()\n",
    "nmf_labelled_topics_df.rename(columns = {x: x + \"_words\" for x in nmf_labelled_topics_df.columns}, \n",
    "                              inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding columns with topic\n",
    "for column in [x.replace(\"_words\", \"_topic\") for x in nmf_labelled_topics_df.columns]:\n",
    "    nmf_labelled_topics_df[column] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually checking topics\n",
    "num_topics = max(num_topics_dict.values())\n",
    "for entity in key_entities_small:\n",
    "    print(f\"Entity {entity}:\")\n",
    "    print(\"\\n\")\n",
    "    for topic in range(num_topics):\n",
    "        if pd.notnull(nmf_labelled_topics_df.loc[topic, entity + '_words']):\n",
    "            print(f\"Topic number {topic}:\")\n",
    "            print(f\"Words: {nmf_labelled_topics_df.loc[topic, entity + '_words']}\")\n",
    "            print(f\"Topic: {nmf_labelled_topics_df.loc[topic, entity + '_topic']}\")\n",
    "            topic_label = input(\"Topic: \")\n",
    "            if topic_label == \"n/a\":\n",
    "                topic_label = np.nan\n",
    "                nmf_labelled_topics_df.loc[topic, entity + \"_topic\"] = topic_label\n",
    "            else:\n",
    "                nmf_labelled_topics_df.loc[topic, entity + \"_topic\"] = topic_label\n",
    "        else:\n",
    "            continue\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to CSV\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "nmf_labelled_topics_df.to_csv(\"Outputs/Articles/Topic Models/nmf.csv\")\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3: <a class=\"anchor\" id=\"section_2_3\"></a> Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating nlp\n",
    "nlp = spacy.load(\"de_core_news_md\", disable = [\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to return Spacy vectors\n",
    "def get_spacy_vectors(tokens):\n",
    "    tokens = [token_split for token in tokens for token_split in token.split(\"_\")]\n",
    "    tokens_string = \" \".join(tokens)\n",
    "    dim_x = 300\n",
    "    try:\n",
    "        doc = nlp(tokens_string)\n",
    "        dim_y = len(doc)\n",
    "        vectors = np.empty((dim_y, dim_x))\n",
    "        for index, token in enumerate(doc):\n",
    "            try:\n",
    "                vector = token.vector\n",
    "                vectors[index] = vector\n",
    "            except:\n",
    "                vectors[index] = 0\n",
    "    except:\n",
    "        vectors = np.zeros((1, dim_x))\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating embedding dataframe\n",
    "df_embed = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 15886/15886 [00:40<00:00, 388.95it/s]\n"
     ]
    }
   ],
   "source": [
    "#Getting Spacy vectors\n",
    "df_embed[\"passage_spacy_vectors\"] = df_embed[\"passage_tokens_lemmatized\"].progress_apply(lambda x: get_spacy_vectors(x).mean(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to identify topics of criticism for given entity\n",
    "def get_topics_of_criticism_kmeans(entity, num_clusters, tfidf):\n",
    "    #Filtering by entity\n",
    "    data = df_embed[df_embed[\"entity_name\"] == entity]\n",
    "    \n",
    "    #Fitting KMeans\n",
    "    km = KMeans(\n",
    "        n_clusters = num_clusters, \n",
    "        init = \"random\",\n",
    "        n_init = 10, \n",
    "        max_iter = 300, \n",
    "        random_state = 1)\n",
    "    clusters = km.fit_predict(data[\"passage_spacy_vectors\"].to_list())\n",
    "    \n",
    "    #Assigning clusters\n",
    "    indeces = data.index\n",
    "    df_embed[\"cluster\"] = np.nan\n",
    "    df_embed.loc[indeces, \"cluster\"] = clusters\n",
    "    \n",
    "    if tfidf == False:\n",
    "        #Finding most common words via CountVectorizer\n",
    "        cv = CountVectorizer()\n",
    "        all_topics = []\n",
    "        for x in range(num_clusters):\n",
    "            cluster = df_embed[(df_embed[\"entity_name\"] == entity) & (df_embed[\"cluster\"] == x)][\"passage_tokens_lemmatized\"]\n",
    "            cv.fit(cluster.apply(lambda x: \" \".join(x)))\n",
    "            passage_cv = cv.transform(cluster.apply(lambda x: \" \".join(x)))\n",
    "            sum_words = passage_cv.sum(axis = 0)\n",
    "            freq_words = [(word, sum_words[0, index]) for word, index in cv.vocabulary_.items()]\n",
    "            freq_words = sorted(freq_words, key = lambda x: x[1], reverse = True)\n",
    "            top_15_words = freq_words[:15]\n",
    "            top_15_words = [x[0] for x in top_15_words]\n",
    "            all_topics.append(top_15_words)\n",
    "    else:\n",
    "        #Finding most common words via TF-IDF\n",
    "        tf = TfidfVectorizer()\n",
    "        all_topics = []\n",
    "        for x in range(num_clusters):\n",
    "            cluster = df_embed[(df_embed[\"entity_name\"] == entity) & (df_embed[\"cluster\"] == x)][\"passage_tokens_lemmatized\"]\n",
    "            tf.fit(cluster.apply(lambda x: \" \".join(x)))\n",
    "            passage_tf = tf.transform(cluster.apply(lambda x: \" \".join(x)))\n",
    "            sum_words = passage_tf.sum(axis = 0)\n",
    "            freq_words = [(word, sum_words[0, index]) for word, index in tf.vocabulary_.items()]\n",
    "            freq_words = sorted(freq_words, key = lambda x: x[1], reverse = True)\n",
    "            top_15_words = freq_words[:15]\n",
    "            top_15_words = [x[0] for x in top_15_words]\n",
    "            all_topics.append(top_15_words)\n",
    "    \n",
    "    return all_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to save topics\n",
    "def generate_kmeans_topics(num_clusters, tfidf, entities):\n",
    "    topics_df = pd.DataFrame(index = [x for x in range(num_clusters)])\n",
    "    for entity in entities:\n",
    "        try:\n",
    "            all_topics = get_topics_of_criticism_kmeans(entity, \n",
    "                                                        num_clusters,\n",
    "                                                        tfidf)\n",
    "            topics_df[entity] = all_topics\n",
    "        except:\n",
    "            topics_df[entity] = 0\n",
    "    return topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting topics\n",
    "kmeans_topics_df = generate_kmeans_topics(3, True, key_entities_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: <a class=\"anchor\" id=\"chapter3\"></a> Visualization Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1: <a class=\"anchor\" id=\"section_3_1\"></a> Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to restructure NMF dataframe, which contains all words and how related they are to a certain topic\n",
    "def restructure_df(component_df, nmf_labelled_topics_df, entity):\n",
    "    topic_map = {str(index): topic for index, topic in enumerate(nmf_labelled_topics_df[entity+\"_topic\"])}\n",
    "    df_restructured = component_df.T.reset_index().rename(columns = {\"index\": \"word\", \n",
    "                                                                     0: \"topic_0\",\n",
    "                                                                     1: \"topic_1\",\n",
    "                                                                     2: \"topic_2\"})\n",
    "    df_restructured[\"topic_index\"] = np.array(df_restructured.loc[:,\"topic_0\":\"topic_2\"]).argmax(axis = 1)\n",
    "    df_restructured[\"word_weight\"] = np.array(df_restructured.loc[:,\"topic_0\":\"topic_2\"]).max(axis = 1)\n",
    "    df_restructured[\"topic\"] = df_restructured[\"topic_index\"].apply(lambda x: topic_map[str(x)])\n",
    "    return df_restructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to fit PCA or TSNE on NMF dataframe to project topics onto two dimensions\n",
    "def fit_dim_red(df_restructured, pca, entity):\n",
    "    if pca:\n",
    "        #Fitting PCA for dimensionality reduction into 2 dimensions\n",
    "        pca = PCA(n_components = 2, random_state = 1)\n",
    "        components = pca.fit_transform(df_restructured.loc[:,\"topic_0\":\"topic_2\"])\n",
    "\n",
    "        #Creating dataframe \n",
    "        df_reduced = pd.DataFrame(data = components, \n",
    "                                  columns = [\"component_1\", \n",
    "                                             \"component_2\"])\n",
    "        df_reduced[\"entity\"] = entity\n",
    "        df_reduced[\"word\"] = df_restructured[\"word\"]\n",
    "        df_reduced[\"topic\"] = df_restructured[\"topic\"]\n",
    "        \n",
    "    else:\n",
    "        #Fitting TSNE for dimensionality reduction into 2 dimensions\n",
    "        tsne = TSNE(n_components = 2, random_state = 1)\n",
    "        components = tsne.fit_transform(df_restructured.loc[:,\"topic_0\":\"topic_2\"])\n",
    "\n",
    "        #Creating dataframe \n",
    "        df_reduced = pd.DataFrame(data = components, \n",
    "                                  columns = [\"component_1\", \n",
    "                                             \"component_2\"])\n",
    "        df_reduced[\"entity\"] = entity\n",
    "        df_reduced[\"word\"] = df_restructured[\"word\"]\n",
    "        df_reduced[\"word_weight\"] = df_restructured[\"word_weight\"]\n",
    "        df_reduced[\"topic\"] = df_restructured[\"topic\"]\n",
    "        \n",
    "    return df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "for entity, component_df in zip(key_entities_small, nmf_components_dfs):\n",
    "    #Calculate restructured and reduced dataframes\n",
    "    df_restructured = restructure_df(component_df, nmf_labelled_topics_df, entity)\n",
    "    df_reduced = fit_dim_red(df_restructured, False, entity)\n",
    "    \n",
    "    #Save to CSV\n",
    "    df_restructured.to_csv(f\"Outputs/Articles/Topic Models/Dimensionality Reduction/restructured_{entity}.csv\")\n",
    "    df_reduced.to_csv(f\"Outputs/Articles/Topic Models/Dimensionality Reduction/reduced_{entity}.csv\")\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2: <a class=\"anchor\" id=\"section_3_2\"></a> Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to check whether word is closely related to topic\n",
    "def check_topic_affiliation(df, word, topic):\n",
    "    if df.loc[topic, word] > 0.05:\n",
    "        return df.loc[topic, word]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to check whether word is closely related to another word\n",
    "def check_word_affiliation(df, word, topic):\n",
    "    if df.loc[topic, word] > 0.15:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to calulate network dataframe, which contains all words and how related they are to each other and another topic\n",
    "def calculate_network(entity, component_df, labelled_topics_df):\n",
    "    #Getting corpus words\n",
    "    words = component_df.columns\n",
    "    key_words = [x for x in words if any(component_df[x] > 0.05)]\n",
    "    key_word_index_map = {key: value for value, key in enumerate(key_words)}\n",
    "    \n",
    "    #Getting topics\n",
    "    topics = labelled_topics_df[entity + \"_topic\"][labelled_topics_df[entity + \"_topic\"].notnull()]\n",
    "    \n",
    "    #Getting combos of all key corpus words\n",
    "    combos = list(itertools.combinations(key_words, 2))\n",
    "    \n",
    "    #Creating dataframe with key corpus word pairs\n",
    "    df_words_network = pd.DataFrame(data = {\"word_combos\": combos})\n",
    "    df_words_network[\"word_1\"] = df_words_network[\"word_combos\"].apply(lambda x: list(x)[0])\n",
    "    df_words_network[\"word_2\"] = df_words_network[\"word_combos\"].apply(lambda x: list(x)[1])\n",
    "    df_words_network.drop(\"word_combos\", axis = 1, inplace = True)\n",
    "    \n",
    "    #Getting affiliation of words with topic\n",
    "    new_columns = [word + \"_topic_\" + str(topic) for word in [\"word_1\", \"word_2\"] for topic in range(len(topics))]\n",
    "    columns = [word for word in [\"word_1\", \"word_2\"] for topic in range(len(topics))]\n",
    "    topic_indeces = [topic for word in [\"word_1\", \"word_2\"] for topic in range(len(topics))]\n",
    "    for new_column, column, topic in zip(new_columns, columns, topic_indeces):\n",
    "        df_words_network[new_column] = df_words_network[column].apply(lambda x: check_word_affiliation(component_df, x, topic))\n",
    "        \n",
    "    #Getting topics that both words are related to \n",
    "    new_columns = [\"intersection_topic_\" + str(topic) for topic in range(len(topics))]\n",
    "    columns_1 = [\"word_1_topic_\" + str(topic) for topic in range(len(topics))]\n",
    "    columns_2 = [\"word_2_topic_\" + str(topic) for topic in range(len(topics))]\n",
    "    for new_column, column_1, column_2 in zip(new_columns, columns_1, columns_2):\n",
    "        df_words_network[new_column] = df_words_network.apply(lambda x: 1 if (x[column_1] + x[column_2]) == 2 else 0, axis = 1)\n",
    "    \n",
    "    #Calculating total intersection strength between words\n",
    "    df_words_network[\"intersection_total\"] = df_words_network.apply(lambda x: sum(x[\"intersection_topic_0\":\"intersection_topic_\"+str(len(topics)-1)]), axis = 1)\n",
    "    \n",
    "    #Creating dataframe with key corpus word and topic pairs\n",
    "    df_word_topic_network = pd.DataFrame(data = {\"word\": key_words})\n",
    "    \n",
    "    #Getting affiliation of words with topic\n",
    "    columns = [\"word_topic_\" + str(topic) for topic in range(len(topics))]\n",
    "    topic_indeces = [topic for topic in range(len(topics))]\n",
    "    for column, topic in zip(columns, topic_indeces):\n",
    "        df_word_topic_network[column] = df_word_topic_network[\"word\"].apply(lambda x: check_topic_affiliation(component_df, x, topic))\n",
    "    \n",
    "    return df_words_network, df_word_topic_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping through entities\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "for entity, component_df in zip(key_entities_small, nmf_components_dfs):\n",
    "    #Calculate network dataframes\n",
    "    df_words_network, df_word_topic_network = calculate_network(entity, component_df, nmf_labelled_topics_df)\n",
    "    \n",
    "    #Save to CSV\n",
    "    df_words_network.to_csv(f\"Outputs/Articles/Topic Models/Network/words_network_{entity}.csv\")\n",
    "    df_word_topic_network.to_csv(f\"Outputs/Articles/Topic Models/Network/word_topic_network_{entity}.csv\")\n",
    "    component_df.to_csv(f\"Outputs/Articles/Topic Models/Network/component_df_{entity}.csv\")\n",
    "os.chdir(\"Notebooks/Articles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
